"""
æ•°æ®ç‚¼é‡‘å·¥åŠ - æŠ–éŸ³ç”µå•†æ•°æ®ETLå¼•æ“
åŸºäºPolarsçš„é«˜æ€§èƒ½æ•°æ®å¤„ç†æ ¸å¿ƒ

ä½œè€…: ç®¡é“æ¶æ„å¸ˆ
ç†å¿µ: ä»»ä½•Pandasèƒ½åšçš„ï¼ŒPolarséƒ½èƒ½åšå¾—æ›´å¿«ã€æ›´çœå†…å­˜
"""

import polars as pl
import re
from pathlib import Path
from typing import Dict, Optional, Union, List
import logging
import chardet
import openpyxl
import pandas as pd
import yaml

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def detect_encoding(file_path: Union[str, Path]) -> str:
    """
    æ™ºèƒ½æ£€æµ‹æ–‡ä»¶ç¼–ç 
    
    è§£å†³ç¼–ç é—®é¢˜ï¼šè‰å¦ˆå¦ˆå¯¼å‡ºæ–‡ä»¶å¯èƒ½ä½¿ç”¨GBKã€GB2312æˆ–UTF-8-SIGç¼–ç 
    """
    file_path = Path(file_path)
    
    # è¯»å–æ–‡ä»¶çš„å‰å‡ KBè¿›è¡Œç¼–ç æ£€æµ‹
    with open(file_path, 'rb') as f:
        raw_data = f.read(10240)  # è¯»å–å‰10KB
    
    # ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
    result = chardet.detect(raw_data)
    detected_encoding = result['encoding']
    confidence = result['confidence']
    
    logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç¼–ç : {detected_encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
    
    # ç¼–ç ä¼˜å…ˆçº§åˆ—è¡¨ï¼šä¼˜å…ˆå°è¯•å¸¸è§çš„ä¸­æ–‡ç¼–ç 
    encoding_candidates = [
        'utf-8-sig',  # å¸¦BOMçš„UTF-8ï¼ˆExcelå¯¼å‡ºå¸¸ç”¨ï¼‰
        'utf-8',      # æ ‡å‡†UTF-8
        'gbk',        # ç®€ä½“ä¸­æ–‡GBK
        'gb2312',     # ç®€ä½“ä¸­æ–‡GB2312
        'gb18030',    # æ‰©å±•çš„ä¸­æ–‡ç¼–ç 
        detected_encoding  # chardetæ£€æµ‹çš„ç¼–ç 
    ]
    
    # å»é‡å¹¶è¿‡æ»¤None
    encoding_candidates = list(dict.fromkeys(filter(None, encoding_candidates)))
    
    # æµ‹è¯•æ¯ç§ç¼–ç 
    for encoding in encoding_candidates:
        try:
            with open(file_path, 'r', encoding=encoding) as f:
                # å°è¯•è¯»å–å‰å‡ è¡ŒéªŒè¯ç¼–ç æ˜¯å¦æ­£ç¡®
                for _ in range(10):
                    line = f.readline()
                    if not line:
                        break
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«å¸¸è§çš„ä¸­æ–‡å…³é”®è¯
                    if any(keyword in line for keyword in ['å•†å“', 'é”€é‡', 'æ¦œ', 'åº“', 'æŠ–éŸ³']):
                        logger.info(f"ä½¿ç”¨ç¼–ç : {encoding}")
                        return encoding
                
                # å³ä½¿æ²¡æœ‰å…³é”®è¯ï¼Œå¦‚æœèƒ½æ­£å¸¸è¯»å–ä¹Ÿè®¤ä¸ºç¼–ç æ­£ç¡®
                logger.info(f"ä½¿ç”¨ç¼–ç : {encoding}")
                return encoding
                
        except (UnicodeDecodeError, UnicodeError):
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œé»˜è®¤ä½¿ç”¨utf-8
    logger.warning("æ— æ³•æ£€æµ‹æ–‡ä»¶ç¼–ç ï¼Œä½¿ç”¨é»˜è®¤çš„utf-8")
    return 'utf-8'


def parse_messy_file(file_path: Union[str, Path]) -> Dict[str, pl.DataFrame]:
    """
    ä½¿ç”¨pandas+polarsæ™ºèƒ½è§£ææ··ä¹±çš„æ–‡ä»¶ - é‡æ„ç‰ˆ
    
    ğŸ—ï¸ ç®¡é“æ¶æ„å¸ˆæ ¸å¿ƒé‡æ„ï¼š
    1. ä½¿ç”¨pandaså¼ºå¤§çš„æ–‡ä»¶è¯»å–èƒ½åŠ›è‡ªåŠ¨å¤„ç†ç¼–ç å’Œæ ¼å¼
    2. æ™ºèƒ½å¤šè¡¨æ£€æµ‹å’Œè¡¨å¤´è¯†åˆ«
    3. ç»“åˆé…ç½®æ–‡ä»¶çš„å­—æ®µæ˜ å°„
    4. æœ€ç»ˆè½¬æ¢ä¸ºé«˜æ€§èƒ½Polars DataFrame
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„ï¼ˆCSV/Excelï¼‰
        
    Returns:
        Dict[str, pl.DataFrame]: è¡¨ååˆ°DataFrameçš„æ˜ å°„
    """
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    logger.info(f"ğŸ—ï¸ æ™ºèƒ½è§£æå¼•æ“å¯åŠ¨ - è§£ææ–‡ä»¶: {file_path}")
    
    tables = {}
    
    try:
        # Step 1: æ™ºèƒ½æ£€æµ‹æ–‡ä»¶ç±»å‹å’Œè¯»å–ç­–ç•¥
        file_type = detect_file_type(file_path)
        
        if file_type == 'excel':
            tables = _parse_excel_with_pandas(file_path)
        else:  # csv
            tables = _parse_csv_with_pandas(file_path)
            
    except Exception as e:
        logger.error(f"æ™ºèƒ½è§£æå¤±è´¥: {e}")
        # é™çº§åˆ°åŸå§‹è§£ææ–¹æ³•
        logger.info("é™çº§ä½¿ç”¨åŸå§‹CSVè§£ææ–¹æ³•")
        return _fallback_csv_parse(file_path)
    
    logger.info(f"ğŸ¯ æ™ºèƒ½è§£æå®Œæˆï¼Œå…±å‘ç° {len(tables)} ä¸ªè¡¨")
    
    if not tables:
        logger.warning("æ™ºèƒ½è§£ææœªæ‰¾åˆ°è¡¨æ ¼ï¼Œå°è¯•é™çº§è§£æ")
        return _fallback_csv_parse(file_path)
    
    return tables


def _parse_excel_with_pandas(file_path: Path) -> Dict[str, pl.DataFrame]:
    """
    ä½¿ç”¨pandasè§£æExcelæ–‡ä»¶ï¼Œæ”¯æŒå¤šå·¥ä½œè¡¨å’Œæ™ºèƒ½è¡¨æ ¼æ£€æµ‹
    """
    logger.info(f"ä½¿ç”¨pandasè§£æExcelæ–‡ä»¶: {file_path}")
    tables = {}
    
    try:
        # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
        excel_file = pd.ExcelFile(file_path)
        logger.info(f"å‘ç° {len(excel_file.sheet_names)} ä¸ªå·¥ä½œè¡¨: {excel_file.sheet_names}")
        
        for sheet_name in excel_file.sheet_names:
            try:
                # è¯»å–å·¥ä½œè¡¨æ•°æ®ï¼Œç¡®ä¿å­—ç¬¦ä¸²ç±»å‹ä»¥ä¾¿åç»­stripå¤„ç†
                df_pandas = pd.read_excel(file_path, sheet_name=sheet_name, header=None, dtype=str)
                
                if df_pandas.empty:
                    logger.warning(f"å·¥ä½œè¡¨ '{sheet_name}' ä¸ºç©º")
                    continue
                
                # æ™ºèƒ½æ£€æµ‹è¡¨æ ¼ç»“æ„
                detected_tables = _detect_tables_in_sheet(df_pandas, sheet_name)
                
                # è½¬æ¢ä¸ºPolars DataFrame
                for table_name, table_df in detected_tables.items():
                    try:
                        # è½¬æ¢ä¸ºPolars
                        polars_df = pl.from_pandas(table_df)
                        tables[table_name] = polars_df
                        logger.info(f"âœ… æˆåŠŸè§£æè¡¨ '{table_name}': {len(polars_df)} è¡Œ x {len(polars_df.columns)} åˆ—")
                    except Exception as e:
                        logger.error(f"è½¬æ¢è¡¨ '{table_name}' åˆ°Polarså¤±è´¥: {e}")
                        
            except Exception as e:
                logger.warning(f"è¯»å–å·¥ä½œè¡¨ '{sheet_name}' å¤±è´¥: {e}")
                continue
                
    except Exception as e:
        logger.error(f"è§£æExcelæ–‡ä»¶å¤±è´¥: {e}")
        raise
    
    return tables


def _parse_csv_with_pandas(file_path: Path) -> Dict[str, pl.DataFrame]:
    """
    ä½¿ç”¨pandasè§£æCSVæ–‡ä»¶ï¼Œæ”¯æŒæ™ºèƒ½ç¼–ç æ£€æµ‹å’Œå¤šè¡¨ç»“æ„
    """
    logger.info(f"ä½¿ç”¨pandasè§£æCSVæ–‡ä»¶: {file_path}")
    
    # Step 1: æ™ºèƒ½ç¼–ç æ£€æµ‹
    encoding = detect_encoding(file_path)
    
    try:
        # Step 2: ä½¿ç”¨æ‰‹åŠ¨é€è¡Œè§£æå¤„ç†å¤šè¡¨æ ¼å¼
        # å¦‚æœæ£€æµ‹åˆ°çš„ç¼–ç ä¸æ˜¯utf-8-sigï¼Œä½†æ–‡ä»¶å¯èƒ½æœ‰BOMï¼Œä¼˜å…ˆä½¿ç”¨utf-8-sig
        if encoding in ['utf-8', 'UTF-8']:
            encoding = 'utf-8-sig'
        
        logger.info("ä½¿ç”¨é€è¡Œè§£ææ–¹å¼å¤„ç†å¤šè¡¨æ ¼å¼æ–‡ä»¶")
        
        # æ‰‹åŠ¨è¯»å–æ–‡ä»¶è¡Œå¹¶æ™ºèƒ½åˆ†éš”ç¬¦æ£€æµ‹
        with open(file_path, 'r', encoding=encoding) as f:
            lines = [line.strip() for line in f.readlines() if line.strip()]
        
        # æ£€æµ‹åˆ†éš”ç¬¦
        separator = _detect_separator_from_lines(lines)
        logger.info(f"æ£€æµ‹åˆ°åˆ†éš”ç¬¦: '{repr(separator)}'")
        
        # è§£æä¸ºDataFrameæ ¼å¼çš„æ•°æ®
        df_pandas = _parse_multiformat_lines(lines, separator)
        
        logger.info(f"æ‰‹åŠ¨è§£ææˆåŠŸ: {len(df_pandas)} è¡Œ x {len(df_pandas.columns)} åˆ—")
        
        # Step 3: æ™ºèƒ½æ£€æµ‹å¤šè¡¨ç»“æ„
        tables = _detect_tables_in_sheet(df_pandas, "main")
        
        # Step 4: è½¬æ¢ä¸ºPolars DataFrame
        polars_tables = {}
        for table_name, table_df in tables.items():
            try:
                polars_df = pl.from_pandas(table_df)
                polars_tables[table_name] = polars_df
                logger.info(f"âœ… æˆåŠŸè§£æè¡¨ '{table_name}': {len(polars_df)} è¡Œ x {len(polars_df.columns)} åˆ—")
            except Exception as e:
                logger.error(f"è½¬æ¢è¡¨ '{table_name}' åˆ°Polarså¤±è´¥: {e}")
        
        return polars_tables
        
    except Exception as e:
        logger.error(f"pandasè¯»å–CSVå¤±è´¥: {e}")
        raise


def _detect_separator_from_lines(lines: List[str]) -> str:
    """
    ä»æ–‡æœ¬è¡Œä¸­æ™ºèƒ½æ£€æµ‹åˆ†éš”ç¬¦
    """
    separators = ['\t', ',', ';', '|']
    separator_scores = {}
    
    # åˆ†æå‰å‡ è¡Œæ¥ç¡®å®šæœ€å¯èƒ½çš„åˆ†éš”ç¬¦
    for sep in separators:
        scores = []
        for line in lines[:10]:  # åªæ£€æŸ¥å‰10è¡Œ
            if sep in line:
                parts = line.split(sep)
                # è¯„åˆ†ï¼šåˆ—æ•° * éç©ºåˆ—æ•°å æ¯”
                non_empty_parts = len([p for p in parts if p.strip()])
                if len(parts) > 1:
                    score = len(parts) * (non_empty_parts / len(parts))
                    scores.append(score)
        
        if scores:
            separator_scores[sep] = sum(scores) / len(scores)
    
    if not separator_scores:
        return '\t'  # é»˜è®¤åˆ¶è¡¨ç¬¦
    
    # è¿”å›å¾—åˆ†æœ€é«˜çš„åˆ†éš”ç¬¦
    best_separator = max(separator_scores.keys(), key=lambda x: separator_scores[x])
    return best_separator


def _parse_multiformat_lines(lines: List[str], separator: str) -> pd.DataFrame:
    """
    è§£æå¤šè¡¨æ ¼å¼çš„æ–‡æœ¬è¡Œä¸ºç»Ÿä¸€çš„DataFrame
    """
    # æ‰¾åˆ°æœ€å¤§åˆ—æ•°
    max_columns = 0
    for line in lines:
        if separator in line:
            parts = line.split(separator)
            max_columns = max(max_columns, len(parts))
    
    # è§£ææ‰€æœ‰è¡Œï¼Œç»Ÿä¸€åˆ—æ•°
    parsed_rows = []
    for line in lines:
        if separator in line:
            parts = line.split(separator)
            # è¡¥é½åˆ—æ•°
            while len(parts) < max_columns:
                parts.append('')
            parsed_rows.append([p.strip() for p in parts[:max_columns]])
        else:
            # å•åˆ—è¡Œï¼ˆé€šå¸¸æ˜¯è¡¨æ ‡é¢˜ï¼‰
            row = [line.strip()]
            while len(row) < max_columns:
                row.append('')
            parsed_rows.append(row)
    
    # åˆ›å»ºDataFrame
    if parsed_rows:
        columns = [f"col_{i}" for i in range(max_columns)]
        df = pd.DataFrame(parsed_rows, columns=columns)
        return df
    else:
        return pd.DataFrame()


def _detect_tables_in_sheet(df_pandas: pd.DataFrame, sheet_name: str) -> Dict[str, pd.DataFrame]:
    """
    åœ¨pandas DataFrameä¸­æ™ºèƒ½æ£€æµ‹å¤šä¸ªè¡¨æ ¼ - é‡æ„ç‰ˆ
    ä½¿ç”¨æ›´å¯é çš„è¡¨å¤´è¯†åˆ«ç®—æ³•
    
    Args:
        df_pandas: pandas DataFrameï¼ˆåŸå§‹æ•°æ®ï¼‰
        sheet_name: å·¥ä½œè¡¨åç§°
        
    Returns:
        Dict[str, pd.DataFrame]: æ£€æµ‹åˆ°çš„è¡¨æ ¼å­—å…¸
    """
    logger.info(f"å¼€å§‹é‡æ„ç‰ˆè¡¨æ ¼ç»“æ„æ£€æµ‹ - å·¥ä½œè¡¨: {sheet_name}")
    tables = {}
    
    # å®šä¹‰å¯èƒ½çš„è¡¨å¤´åˆ—å…³é”®è¯ï¼Œè¶Šå¤šè¶Šå¥½
    HEADER_KEYWORDS = {
        'å•†å“', 'é”€é‡', 'é”€å”®é¢', 'ä½£é‡‘', 'è½¬åŒ–ç‡', 'é“¾æ¥', 'åˆ†ç±»',
        'å•†å“æ ‡é¢˜', 'å•†å“é“¾æ¥', 'å•†å“ä»·æ ¼', 'åº—é“º', 'å“ç‰Œ', 'ç±»ç›®',
        'ä½£é‡‘æ¯”ä¾‹', 'ç›´æ’­é”€å”®é¢', 'å•†å“å¡é”€å”®é¢', 'è¿‘30å¤©é”€é‡',
        'å‘¨é”€é‡', 'è¿‘1å¹´é”€é‡', '30å¤©è½¬åŒ–ç‡', 'ä¸Šæ¶æ—¶é—´', 'è¾¾äººæ˜µç§°'
    }
    
    possible_header_indices = []
    
    # 1. é¢„æ‰«æï¼Œæ‰¾åˆ°æ‰€æœ‰å¯èƒ½æ˜¯è¡¨å¤´çš„è¡Œçš„ç´¢å¼•
    for idx, row in df_pandas.iterrows():
        # ç¡®ä¿æ‰€æœ‰å€¼éƒ½stripå¤„ç†
        row_values = {str(cell).strip() for cell in row.values if pd.notna(cell) and str(cell).strip()}
        
        # å¦‚æœæŸä¸€è¡ŒåŒ…å«è¶³å¤Ÿå¤šçš„å…³é”®è¯ï¼Œå°±è®¤ä¸ºå®ƒå¯èƒ½æ˜¯è¡¨å¤´
        keyword_matches = len(HEADER_KEYWORDS.intersection(row_values))
        non_empty_cells = len(row_values)
        
        # åŠ¨æ€æ¡ä»¶åˆ¤æ–­ï¼š
        # æ¡ä»¶1ï¼šåŒ¹é…å…³é”®è¯æ•° >= 2 (é™ä½è¦æ±‚)
        # æ¡ä»¶2ï¼šéç©ºå•å…ƒæ ¼æ•°é‡ >= 4 (æ”¯æŒå°è¡¨æ ¼)
        # æ¡ä»¶3ï¼šéç©ºå•å…ƒæ ¼å æ¯” > 40% (æ›´å®½æ¾)
        # æ¡ä»¶4ï¼šç‰¹æ®Šæƒ…å†µï¼šå¦‚æœåŒ…å«"æ’å"+"å•†å“"+"ä½£é‡‘æ¯”ä¾‹"ç­‰æ ¸å¿ƒç»„åˆï¼Œé™ä½è¦æ±‚
        
        core_combo = {'æ’å', 'å•†å“', 'ä½£é‡‘æ¯”ä¾‹'}.intersection(row_values)
        is_core_combo = len(core_combo) >= 2
        
        if ((keyword_matches >= 2 and non_empty_cells >= 4) or 
            (keyword_matches >= 1 and non_empty_cells >= 5) or
            (is_core_combo and non_empty_cells >= 3)):
            cell_ratio = non_empty_cells / len(row.values)
            if cell_ratio > 0.4:
                possible_header_indices.append(idx)
                logger.debug(f"å‘ç°å¯èƒ½çš„è¡¨å¤´è¡Œ {idx}: åŒ¹é…{keyword_matches}ä¸ªå…³é”®è¯ï¼Œ{non_empty_cells}ä¸ªéç©ºå•å…ƒæ ¼ï¼Œæ ¸å¿ƒç»„åˆ:{is_core_combo}")
    
    if not possible_header_indices:
        logger.error("åœ¨å·¥ä½œè¡¨ä¸­æœªèƒ½è¯†åˆ«å‡ºä»»ä½•æœ‰æ•ˆçš„è¡¨å¤´è¡Œï¼")
        logger.warning("ä»¥ä¸‹æ˜¯å·¥ä½œè¡¨çš„å‰10è¡Œå†…å®¹ï¼Œè¯·æ£€æŸ¥å…¶æ ¼å¼ï¼š")
        logger.warning(df_pandas.head(10).to_string())
        
        # é™çº§å¤„ç†ï¼šå°è¯•å¯»æ‰¾ä»»ä½•åŒ…å«å…³é”®è¯çš„è¡Œ
        for idx, row in df_pandas.iterrows():
            row_values = {str(cell).strip() for cell in row.values if pd.notna(cell) and str(cell).strip()}
            if len(HEADER_KEYWORDS.intersection(row_values)) >= 1 and len(row_values) >= 3:
                possible_header_indices.append(idx)
        
        if not possible_header_indices:
            return {}

    logger.info(f"è¯†åˆ«åˆ° {len(possible_header_indices)} ä¸ªå¯èƒ½çš„è¡¨å¤´ï¼Œä½äºè¡Œ: {possible_header_indices}")

    # 2. æ ¹æ®æ‰¾åˆ°çš„è¡¨å¤´è¡Œï¼Œåˆ‡åˆ†æ•°æ®å—
    for i, header_idx in enumerate(possible_header_indices):
        # ç¡®å®šæ•°æ®å—çš„èµ·æ­¢è¡Œ
        start_data_idx = header_idx + 1
        end_data_idx = possible_header_indices[i+1] if i + 1 < len(possible_header_indices) else len(df_pandas)
        
        # æå–æ•°æ®å—
        table_df = df_pandas.iloc[start_data_idx:end_data_idx].copy()
        
        # æ¸…ç†æ‰å®Œå…¨æ˜¯ç©ºå€¼çš„è¡Œ
        table_df.dropna(how='all', inplace=True)

        if table_df.empty:
            continue

        # è®¾ç½®æ­£ç¡®çš„è¡¨å¤´ï¼ˆç¡®ä¿stripå¤„ç†ï¼‰
        header = [str(col).strip() if pd.notna(col) and str(col).strip() else f"col_{j}" 
                 for j, col in enumerate(df_pandas.iloc[header_idx].values)]
        
        # ç¡®ä¿åˆ—æ•°åŒ¹é…
        header = header[:len(table_df.columns)]
        
        # ç¡®ä¿åˆ—åå”¯ä¸€æ€§
        unique_header = []
        col_counts = {}
        for col in header:
            if col in col_counts:
                col_counts[col] += 1
                unique_col = f"{col}_{col_counts[col]}"
            else:
                col_counts[col] = 0
                unique_col = col
            unique_header.append(unique_col)
        
        table_df.columns = unique_header
        
        # ç¡®å®šè¡¨åï¼ˆæŸ¥æ‰¾è¡¨å¤´å‰é¢çš„æ ‡é¢˜è¡Œï¼‰
        table_name = f"æ•°æ®è¡¨_{i+1}"
        if header_idx > 0:
            # æ£€æŸ¥å‰é¢å‡ è¡Œæ˜¯å¦æœ‰è¡¨å
            for j in range(max(0, header_idx-3), header_idx):
                title_candidate = ' '.join([str(cell).strip() for cell in df_pandas.iloc[j].values 
                                          if pd.notna(cell) and str(cell).strip()])
                if title_candidate:
                    # æ‰©å±•è¡¨åå…³é”®è¯æ£€æŸ¥
                    table_keywords = ['é”€é‡æ¦œ', 'å•†å“åº“', 'SKU', 'æŠ–éŸ³', 'ç›´æ’­', 'çƒ­æ¨æ¦œ', 'æ½œåŠ›çˆ†å“æ¦œ', 'æŒç»­å¥½è´§æ¦œ', 'å†å²åŒæœŸæ¦œ']
                    if any(keyword in title_candidate for keyword in table_keywords):
                        table_name = title_candidate.strip()
                        break
                    # å¦‚æœåŒ…å«"æ¦œ"æˆ–"åº“"å­—æ ·ï¼Œä¹Ÿè®¤ä¸ºæ˜¯è¡¨å
                    elif any(keyword in title_candidate for keyword in ['æ¦œ', 'åº“']):
                        table_name = title_candidate.strip()
                        break

        tables[table_name] = table_df
        logger.info(f"æˆåŠŸæå–è¡¨æ ¼ '{table_name}'ï¼Œå…± {len(table_df)} è¡Œæ•°æ®ï¼Œ{len(table_df.columns)} åˆ—")
        logger.debug(f"è¡¨æ ¼ '{table_name}' çš„åˆ—å: {list(table_df.columns)[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªåˆ—å
    
    # 3. å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•è¡¨æ ¼ï¼Œå°è¯•é™çº§å¤„ç†
    if not tables and not df_pandas.empty:
        logger.warning("ä½¿ç”¨é™çº§æ–¹æ¡ˆï¼šå°†æ•´ä¸ªæ•°æ®ä½œä¸ºå•è¡¨å¤„ç†")
        # å¯»æ‰¾ç¬¬ä¸€ä¸ªæœ‰è¶³å¤Ÿéç©ºå•å…ƒæ ¼çš„è¡Œä½œä¸ºè¡¨å¤´
        for idx, row in df_pandas.iterrows():
            non_empty_cells = [str(cell).strip() for cell in row.values if pd.notna(cell) and str(cell).strip()]
            if len(non_empty_cells) >= 3:
                table_df = df_pandas.iloc[idx:].copy()
                header = [str(col).strip() if pd.notna(col) and str(col).strip() else f"col_{j}" 
                         for j, col in enumerate(table_df.iloc[0].values)]
                header = header[:len(table_df.columns)]
                table_df.columns = header
                table_df = table_df.iloc[1:].reset_index(drop=True)
                table_df.dropna(how='all', inplace=True)
                
                if not table_df.empty:
                    tables[f"{sheet_name}_æ•°æ®è¡¨"] = table_df
                    logger.info(f"é™çº§å¤„ç†: åˆ›å»ºè¡¨ '{sheet_name}_æ•°æ®è¡¨', {len(table_df)} è¡Œ")
                break
        
    return tables


def _extract_table_data(df_pandas: pd.DataFrame, table_start: int, header_row: int, table_end: int) -> pd.DataFrame:
    """
    ä»pandas DataFrameä¸­æå–å•ä¸ªè¡¨æ ¼çš„æ•°æ®
    """
    try:
        # æå–è¡¨å¤´
        header_data = df_pandas.iloc[header_row]
        headers = [str(cell).strip() if pd.notna(cell) and str(cell).strip() else f"col_{i}" 
                  for i, cell in enumerate(header_data)]
        
        # æå–æ•°æ®è¡Œï¼ˆè¡¨å¤´ä¹‹ååˆ°è¡¨ç»“æŸï¼‰
        data_start = header_row + 1
        if data_start >= table_end:
            return None
        
        table_data = df_pandas.iloc[data_start:table_end].copy()
        
        # è¿‡æ»¤æ‰å®Œå…¨ç©ºè¡Œ
        table_data = table_data.dropna(how='all')
        
        if table_data.empty:
            return None
        
        # è®¾ç½®åˆ—å
        table_data.columns = headers[:len(table_data.columns)]
        
        # é‡ç½®ç´¢å¼•
        table_data = table_data.reset_index(drop=True)
        
        return table_data
        
    except Exception as e:
        logger.error(f"æå–è¡¨æ ¼æ•°æ®å¤±è´¥: {e}")
        return None




def _extract_table_name(text: str) -> str:
    """
    ä»åŒ…å«è¡¨åçš„æ–‡æœ¬ä¸­æå–è¡¨å
    """
    # æå–å…³é”®è¯
    keywords = ['SKUå•†å“åº“', 'æŠ–éŸ³é”€é‡æ¦œ', 'ç›´æ’­é”€é‡æ¦œ', 'å•†å“å¡é”€é‡æ¦œ']
    
    for keyword in keywords:
        if keyword in text:
            return keyword
    
    # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…ï¼Œæå–åŒ…å«å…³é”®è¯çš„éƒ¨åˆ†
    if 'å•†å“åº“' in text:
        return 'SKUå•†å“åº“'
    elif 'é”€é‡æ¦œ' in text:
        if 'æŠ–éŸ³' in text:
            return 'æŠ–éŸ³é”€é‡æ¦œ'
        elif 'ç›´æ’­' in text:
            return 'ç›´æ’­é”€é‡æ¦œ'
        elif 'å•†å“å¡' in text:
            return 'å•†å“å¡é”€é‡æ¦œ'
        else:
            return 'é”€é‡æ¦œ'
    
    # é»˜è®¤è¿”å›æ¸…ç†åçš„æ–‡æœ¬å‰20ä¸ªå­—ç¬¦
    clean_text = re.sub(r'[^\w\u4e00-\u9fff\-_]', '', text)[:20]
    return clean_text if clean_text else 'æœªçŸ¥è¡¨æ ¼'


def _fallback_csv_parse(file_path: Path) -> Dict[str, pl.DataFrame]:
    """
    é™çº§çš„CSVè§£ææ–¹æ³•ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ç”¨ï¼‰
    """
    logger.info("ä½¿ç”¨é™çº§CSVè§£ææ–¹æ³•")
    
    # è¿™é‡Œä¿ç•™åŸæœ‰çš„parse_multi_table_csvé€»è¾‘çš„æ ¸å¿ƒéƒ¨åˆ†
    encoding = detect_encoding(file_path)
    
    try:
        with open(file_path, 'r', encoding=encoding) as f:
            lines = f.readlines()
    except UnicodeDecodeError:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            lines = f.readlines()
    
    # ç®€åŒ–çš„è¡¨æ ¼è¯†åˆ«é€»è¾‘
    tables = {}
    current_table = None
    current_data = []
    
    for line in lines:
        line = line.strip().replace('\ufeff', '')
        if not line or not line.replace(',', '').strip():
            continue
        
        # ç®€å•çš„è¡¨å¤´æ£€æµ‹
        if any(keyword in line for keyword in ['å•†å“', 'é”€é‡', 'æ¦œ', 'åº“', 'SKU']):
            if ',' not in line or line.count(',') <= 3:
                # å¯èƒ½æ˜¯è¡¨å¤´
                if current_table and current_data:
                    try:
                        tables[current_table] = _parse_table_data(current_data)
                    except:
                        pass
                
                current_table = line.split(',')[0].strip()[:20]
                current_data = []
                continue
        
        if current_table and ',' in line:
            current_data.append(line)
    
    # ä¿å­˜æœ€åä¸€ä¸ªè¡¨
    if current_table and current_data:
        try:
            tables[current_table] = _parse_table_data(current_data)
        except:
            pass
    
    return tables


def _parse_table_data(data_lines: List[str]) -> pl.DataFrame:
    """
    è§£æå•ä¸ªè¡¨çš„æ•°æ®
    
    Args:
        data_lines: è¡¨æ•°æ®è¡Œåˆ—è¡¨
        
    Returns:
        pl.DataFrame: è§£æåçš„DataFrame
    """
    if len(data_lines) < 2:
        return pl.DataFrame()
    
    # ç¬¬ä¸€è¡Œæ˜¯åˆ—å
    header = data_lines[0].split(',')
    
    # æ¸…ç†åˆ—åï¼ˆç§»é™¤ç©ºç™½å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
    header = [col.strip() for col in header if col.strip()]
    
    # æ•°æ®è¡Œ
    rows = []
    for line in data_lines[1:]:
        row = line.split(',')
        # ç¡®ä¿è¡Œé•¿åº¦ä¸åˆ—æ•°åŒ¹é…
        while len(row) < len(header):
            row.append('')
        rows.append(row[:len(header)])
    
    if not rows:
        return pl.DataFrame(schema=header)
    
    # åˆ›å»ºDataFrame
    df = pl.DataFrame(rows, schema=header)
    
    return df


def parse_fuzzy_numeric_range(series: pl.Series, unit_multiplier: int = 1) -> Dict[str, pl.Series]:
    """
    è§£ææ¨¡ç³Šæ•°å€¼èŒƒå›´
    
    æ ¸å¿ƒæŒ‘æˆ˜ï¼šå°†"7.5w~10w"è¿™æ ·çš„èŒƒå›´è½¬æ¢ä¸ºmin/max/avgæ•°å€¼
    
    Args:
        series: åŒ…å«æ¨¡ç³ŠèŒƒå›´çš„Series
        unit_multiplier: å•ä½ä¹˜æ•°ï¼ˆw=10000ï¼‰
        
    Returns:
        DictåŒ…å«min/max/avgçš„Series
    """
    min_values = []
    max_values = []
    avg_values = []
    
    # éå†æ¯ä¸ªå€¼è¿›è¡Œå¤„ç†
    for value in series.to_list():
        try:
            if value is None or value == '':
                min_values.append(None)
                max_values.append(None)
                avg_values.append(None)
                continue
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²å¹¶æ¸…ç†
            str_val = str(value).strip()
            str_val = str_val.replace('ä¸‡', 'w').replace('W', 'w').replace('%', '')
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«èŒƒå›´ç¬¦å·
            if '~' in str_val:
                # è§£æèŒƒå›´
                parts = str_val.split('~')
                if len(parts) == 2:
                    min_part = parts[0].strip()
                    max_part = parts[1].strip()
                    
                    # è§£ææœ€å°å€¼
                    if 'w' in min_part:
                        min_val = float(min_part.replace('w', '')) * 10000
                    else:
                        min_val = float(min_part)
                    
                    # è§£ææœ€å¤§å€¼
                    if 'w' in max_part:
                        max_val = float(max_part.replace('w', '')) * 10000
                    else:
                        max_val = float(max_part)
                    
                    min_values.append(min_val)
                    max_values.append(max_val)
                    avg_values.append((min_val + max_val) / 2)
                else:
                    min_values.append(None)
                    max_values.append(None)
                    avg_values.append(None)
            else:
                # å•ä¸€æ•°å€¼
                if 'w' in str_val:
                    val = float(str_val.replace('w', '')) * 10000
                else:
                    # æ£€æŸ¥æ˜¯å¦ä¸ºçº¯æ•°å­—
                    if str_val.replace('.', '').replace('-', '').isdigit():
                        val = float(str_val)
                    else:
                        val = None
                
                if val is not None:
                    min_values.append(val)
                    max_values.append(val)
                    avg_values.append(val)
                else:
                    min_values.append(None)
                    max_values.append(None)
                    avg_values.append(None)
                    
        except Exception as e:
            logger.warning(f"è§£ææ•°å€¼å¤±è´¥: {value}, é”™è¯¯: {e}")
            min_values.append(None)
            max_values.append(None)
            avg_values.append(None)
    
    return {
        'min': pl.Series(min_values, dtype=pl.Float64),
        'max': pl.Series(max_values, dtype=pl.Float64),
        'avg': pl.Series(avg_values, dtype=pl.Float64)
    }


def clean_common_fields(df: pl.DataFrame) -> pl.DataFrame:
    """
    é€šç”¨å­—æ®µæ¸…æ´—å‡½æ•° - ç„¦åœŸé‡æ„ç‰ˆ
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. ä½£é‡‘æ¯”ä¾‹ç™¾åˆ†æ¯”è½¬æ¢
    2. æ¨¡ç³Šæ•°å€¼èŒƒå›´å¤„ç†
    3. ç²¾ç¡®æ•°å€¼ç±»å‹è½¬æ¢
    
    é‡è¦ï¼šåªè¿”å›å¤„ç†åçš„æ•°æ®ï¼Œä¸åŒ…å«åŸå§‹åˆ—
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pl.DataFrame: åªåŒ…å«æ¸…æ´—åæ•°æ®çš„DataFrame
    """
    logger.info("å¼€å§‹é€šç”¨å­—æ®µæ¸…æ´— - ç„¦åœŸé‡æ„ç‰ˆ")
    
    # åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—æ”¶é›†å™¨ï¼Œåªæ”¶é›†å¤„ç†åçš„åˆ—
    processed_columns = []
    
    # 1. å¤„ç†ä½£é‡‘æ¯”ä¾‹
    if 'ä½£é‡‘æ¯”ä¾‹' in df.columns:
        processed_columns.append(
            pl.col('ä½£é‡‘æ¯”ä¾‹')
            .str.replace('%', '')
            .str.replace('ç™¾åˆ†æ¯”', '')
            .cast(pl.Float64, strict=False)
            .truediv(100)
            .alias('ä½£é‡‘æ¯”ä¾‹')
        )
    
    # 2. å¤„ç†æ¨¡ç³Šæ•°å€¼èŒƒå›´å­—æ®µ
    range_fields = ['è¿‘30å¤©é”€é‡', 'å‘¨é”€é‡', 'è¿‘1å¹´é”€é‡', 'é”€å”®é¢', 'è¿‘30å¤©é”€å”®é¢', 'è¿‘1å¹´é”€å”®é¢', 
                   'æ˜¨æ—¥é”€é‡', 'è¿‘90å¤©é”€é‡', 'åŒæœŸé”€é‡', 'å‘¨å¸¦è´§è¾¾äºº', 'å…³è”è¾¾äºº']
    
    for field in range_fields:
        if field in df.columns:
            logger.info(f"å¤„ç†æ¨¡ç³Šæ•°å€¼å­—æ®µ: {field}")
            
            try:
                # ç¡®ä¿æ•°æ®ä¸ºå­—ç¬¦ä¸²ç±»å‹åå†è§£æ
                field_series = df[field].cast(pl.Utf8)
                
                # è§£æèŒƒå›´
                range_data = parse_fuzzy_numeric_range(field_series)
                
                # åªæ·»åŠ å¤„ç†åçš„æ–°åˆ—åˆ°æ”¶é›†å™¨
                processed_columns.extend([
                    range_data['min'].alias(f'{field}_min'),
                    range_data['max'].alias(f'{field}_max'),
                    range_data['avg'].alias(f'{field}_avg')
                ])
                
                logger.info(f"æˆåŠŸå¤„ç†æ¨¡ç³Šæ•°å€¼å­—æ®µ: {field}")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ¨¡ç³Šæ•°å€¼å­—æ®µ {field} å¤±è´¥: {e}")
                logger.warning(f"è·³è¿‡å­—æ®µ {field} çš„èŒƒå›´è§£æ")
                continue
    
    # 3. å¤„ç†ç²¾ç¡®æ•°å€¼å­—æ®µ
    numeric_fields = ['ç›´æ’­é”€å”®é¢', 'å•†å“å¡é”€å”®é¢']
    
    for field in numeric_fields:
        if field in df.columns:
            processed_columns.append(
                pl.col(field)
                .str.replace('%', '')
                .str.replace('ï¼Œ', '')
                .cast(pl.Float64, strict=False)
                .alias(field)
            )
    
    # 4. å¤„ç†è½¬åŒ–ç‡èŒƒå›´
    rate_fields = ['è½¬åŒ–ç‡', '30å¤©è½¬åŒ–ç‡']
    
    for field in rate_fields:
        if field in df.columns:
            try:
                rate_series = df[field].cast(pl.Utf8)
                rate_data = parse_fuzzy_numeric_range(rate_series)
                
                # è½¬åŒ–ç‡éœ€è¦é™¤ä»¥100
                processed_columns.extend([
                    (rate_data['min'] / 100).alias(f'{field}_min'),
                    (rate_data['max'] / 100).alias(f'{field}_max'),
                    (rate_data['avg'] / 100).alias(f'{field}_avg')
                ])
                
                logger.info(f"æˆåŠŸå¤„ç†è½¬åŒ–ç‡å­—æ®µ: {field}")
            except Exception as e:
                logger.error(f"å¤„ç†è½¬åŒ–ç‡å­—æ®µ {field} å¤±è´¥: {e}")
    
    # 5. å¤„ç†éœ€è¦ä¿ç•™çš„åŸå§‹æ–‡æœ¬å­—æ®µ
    text_fields = ['å•†å“', 'å•†å“é“¾æ¥', 'å•†å“åˆ†ç±»', 'å•†å“å¤´å›¾é“¾æ¥', 'è‰å¦ˆå¦ˆå•†å“é“¾æ¥', 
                   'æŠ–éŸ³å•†å“é“¾æ¥', 'å°åº—', 'å“ç‰Œ', 'è‰å¦ˆå¦ˆé“¾æ¥']
    
    for field in text_fields:
        if field in df.columns:
            processed_columns.append(pl.col(field))
    
    # 6. å¤„ç†éœ€è¦ä¿ç•™çš„åŸå§‹æ•°å€¼å­—æ®µï¼ˆå¦‚æ’åï¼‰
    keep_as_is_fields = ['æ’å']
    
    for field in keep_as_is_fields:
        if field in df.columns:
            processed_columns.append(
                pl.col(field).cast(pl.Int64, strict=False).alias(field)
            )
    
    # 7. æ„å»ºæœ€ç»ˆçš„DataFrame - åªåŒ…å«å¤„ç†åçš„åˆ—
    if not processed_columns:
        logger.error("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¯å¤„ç†çš„åˆ—ï¼")
        return pl.DataFrame()  # è¿”å›ç©ºDataFrame
    
    # åˆ›å»ºåªåŒ…å«å¤„ç†åæ•°æ®çš„æ–°DataFrame
    cleaned_df = df.select(processed_columns)
    
    # è¾“å‡ºæœ€ç»ˆçš„åˆ—ä¿¡æ¯
    logger.info(f"æœ€ç»ˆDataFrameåŒ…å« {len(cleaned_df.columns)} åˆ—")
    logger.info(f"æœ€ç»ˆåˆ—å: {cleaned_df.columns}")
    
    # éªŒè¯æ•°æ®
    logger.info(f"æ•°æ®è¡Œæ•°: {len(cleaned_df)}")
    
    # æ˜¾ç¤ºæ•°æ®ç±»å‹åˆ†å¸ƒ
    type_counts = {}
    for col in cleaned_df.columns:
        dtype = str(cleaned_df[col].dtype)
        type_counts[dtype] = type_counts.get(dtype, 0) + 1
    logger.info(f"æ•°æ®ç±»å‹åˆ†å¸ƒ: {type_counts}")
    
    logger.info("é€šç”¨å­—æ®µæ¸…æ´—å®Œæˆ - ç„¦åœŸé‡æ„ç‰ˆ")
    return cleaned_df


def parse_excel_file(file_path: Union[str, Path]) -> Dict[str, pl.DataFrame]:
    """
    è§£æExcelæ–‡ä»¶
    
    æ”¯æŒå¤šä¸ªå·¥ä½œè¡¨ï¼Œæ¯ä¸ªå·¥ä½œè¡¨ä½œä¸ºä¸€ä¸ªè¡¨æ ¼
    
    Args:
        file_path: Excelæ–‡ä»¶è·¯å¾„
        
    Returns:
        Dict[str, pl.DataFrame]: å·¥ä½œè¡¨ååˆ°DataFrameçš„æ˜ å°„
    """
    file_path = Path(file_path)
    if not file_path.exists():
        raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
    
    logger.info(f"å¼€å§‹è§£æExcelæ–‡ä»¶: {file_path}")
    
    tables = {}
    
    try:
        # ä½¿ç”¨openpyxlè¯»å–Excelæ–‡ä»¶
        workbook = openpyxl.load_workbook(file_path, data_only=True)
        
        for sheet_name in workbook.sheetnames:
            logger.info(f"å¤„ç†å·¥ä½œè¡¨: {sheet_name}")
            
            # ä½¿ç”¨Polarsè¯»å–Excelå·¥ä½œè¡¨
            try:
                df = pl.read_excel(file_path, sheet_name=sheet_name)
                
                if len(df) > 0:
                    tables[sheet_name] = df
                    logger.info(f"æˆåŠŸè¯»å–å·¥ä½œè¡¨ '{sheet_name}'ï¼Œ{len(df)} è¡Œ x {len(df.columns)} åˆ—")
                else:
                    logger.warning(f"å·¥ä½œè¡¨ '{sheet_name}' ä¸ºç©º")
                    
            except Exception as e:
                logger.warning(f"è¯»å–å·¥ä½œè¡¨ '{sheet_name}' å¤±è´¥: {e}")
                continue
        
        workbook.close()
        
    except Exception as e:
        logger.error(f"è¯»å–Excelæ–‡ä»¶å¤±è´¥: {e}")
        raise ValueError(f"æ— æ³•è¯»å–Excelæ–‡ä»¶: {e}")
    
    logger.info(f"Excelè§£æå®Œæˆï¼Œå…±å‘ç° {len(tables)} ä¸ªå·¥ä½œè¡¨")
    return tables


def detect_file_type(file_path: Union[str, Path]) -> str:
    """
    æ£€æµ‹æ–‡ä»¶ç±»å‹
    
    Returns:
        'csv' æˆ– 'excel'
    """
    file_path = Path(file_path)
    suffix = file_path.suffix.lower()
    
    if suffix in ['.xlsx', '.xls']:
        return 'excel'
    elif suffix in ['.csv', '.txt']:
        return 'csv'
    else:
        # é»˜è®¤æŒ‰CSVå¤„ç†
        logger.warning(f"æœªçŸ¥æ–‡ä»¶ç±»å‹ {suffix}ï¼ŒæŒ‰CSVå¤„ç†")
        return 'csv'


def process_douyin_export(file_path: Union[str, Path]) -> Optional[pl.DataFrame]:
    """
    æŠ–éŸ³å¯¼å‡ºæ•°æ®ä¸»å¤„ç†æµç¨‹ - å¢å¼ºç‰ˆ
    
    æ•°æ®ç‚¼é‡‘å·¥åŠçš„æ ¸å¿ƒå‡½æ•° - å°†åŸå§‹æ•°æ®ç‚¼æˆçº¯é‡‘
    
    æ–°ç‰¹æ€§ï¼š
    1. æ”¯æŒCSVå’ŒExcelæ–‡ä»¶æ ¼å¼
    2. æ™ºèƒ½ç¼–ç æ£€æµ‹
    3. å¢å¼ºçš„é”™è¯¯å¤„ç†å’Œè°ƒè¯•ä¿¡æ¯
    
    Args:
        file_path: åŸå§‹æ–‡ä»¶è·¯å¾„ï¼ˆCSVæˆ–Excelï¼‰
        
    Returns:
        pl.DataFrame: æ¸…æ´—åçš„ä¸»DataFrame
    """
    logger.info(f"--- [å¼•æ“å…¥å£] å¼€å§‹å¤„ç†æ–‡ä»¶: {file_path} ---")
    logger.info("ğŸ—ï¸ æ•°æ®ç‚¼é‡‘å·¥åŠå¯åŠ¨ - å¼€å§‹æ•°æ®å¤„ç†")
    
    file_path = Path(file_path)
    
    # Step 1: æ£€æµ‹æ–‡ä»¶ç±»å‹å¹¶è§£æ
    file_type = detect_file_type(file_path)
    logger.info(f"æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹: {file_type.upper()}")
    
    try:
        # ä½¿ç”¨messytablesç»Ÿä¸€å¤„ç†CSVå’ŒExcelæ–‡ä»¶
        tables = parse_messy_file(file_path)
    except Exception as e:
        logger.error(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")
        raise ValueError(f"æ–‡ä»¶è§£æå¤±è´¥: {e}")
    
    if not tables:
        raise ValueError("æœªèƒ½è§£æå‡ºä»»ä½•è¡¨æ ¼æ•°æ®")
    
    logger.info(f"æˆåŠŸè§£æå‡º {len(tables)} ä¸ªè¡¨æ ¼: {list(tables.keys())}")
    
    # Step 2: é€‰æ‹©ä¸»è¡¨ï¼ˆä¼˜å…ˆçº§ï¼šæŠ–éŸ³é”€é‡æ¦œ > SKUå•†å“åº“ > åŒ…å«å…³é”®è¯çš„è¡¨ > ç¬¬ä¸€ä¸ªè¡¨ï¼‰
    main_table_name = None
    
    # ä¼˜å…ˆçº§1ï¼šç²¾ç¡®åŒ¹é…
    priority_tables = ['æŠ–éŸ³é”€é‡æ¦œ', 'SKUå•†å“åº“', 'ç›´æ’­é”€é‡æ¦œ', 'å•†å“å¡é”€é‡æ¦œ']
    for priority_name in priority_tables:
        if priority_name in tables and len(tables[priority_name]) > 0:
            main_table_name = priority_name
            break
    
    # ä¼˜å…ˆçº§2ï¼šæ¨¡ç³ŠåŒ¹é…åŒ…å«å…³é”®è¯çš„è¡¨
    if not main_table_name:
        keywords = ['é”€é‡', 'å•†å“', 'æ¦œ', 'åº“']
        for name, df in tables.items():
            if len(df) > 0:
                if any(keyword in name for keyword in keywords):
                    main_table_name = name
                    break
    
    # ä¼˜å…ˆçº§3ï¼šé€‰æ‹©ç¬¬ä¸€ä¸ªéç©ºè¡¨
    if not main_table_name:
        for name, df in tables.items():
            if len(df) > 0:
                main_table_name = name
                break
    
    if not main_table_name:
        raise ValueError("æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸»è¡¨æ•°æ®")
    
    logger.info(f"é€‰æ‹©ä¸»è¡¨: '{main_table_name}' (æ•°æ®è¡Œæ•°: {len(tables[main_table_name])})")
    main_df = tables[main_table_name]
    
    # è¾“å‡ºä¸»è¡¨åŸºæœ¬ä¿¡æ¯
    logger.info(f"ä¸»è¡¨åˆ—å: {main_df.columns}")
    
    # Step 3: æ¸…æ´—ä¸»è¡¨æ•°æ®
    logger.info("å¼€å§‹æ•°æ®æ¸…æ´—å¤„ç†")
    try:
        cleaned_df = clean_common_fields(main_df)
    except Exception as e:
        logger.error(f"æ•°æ®æ¸…æ´—å¤±è´¥: {e}")
        # å¦‚æœæ¸…æ´—å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
        logger.warning("æ•°æ®æ¸…æ´—å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®")
        cleaned_df = main_df
    
    # Step 4: æ•°æ®è´¨é‡æ£€æŸ¥
    logger.info("æ‰§è¡Œæ•°æ®è´¨é‡æ£€æŸ¥")
    
    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    total_rows = len(cleaned_df)
    if total_rows == 0:
        logger.warning("è­¦å‘Šï¼šå¤„ç†åçš„æ•°æ®ä¸ºç©º")
    else:
        logger.info(f"æ•°æ®å¤„ç†å®Œæˆï¼šå…± {total_rows} è¡Œæ•°æ®")
        
        # è¾“å‡ºåˆ—ä¿¡æ¯
        logger.info(f"åˆ—æ•°ï¼š{len(cleaned_df.columns)}")
        logger.info(f"æœ€ç»ˆåˆ—åï¼š{cleaned_df.columns}")
        
        # æ•°æ®ç±»å‹åˆ†æ
        numeric_cols = [col for col in cleaned_df.columns if cleaned_df[col].dtype in [pl.Float64, pl.Int64]]
        logger.info(f"æ•°å€¼åˆ—æ•°é‡: {len(numeric_cols)}")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ç”Ÿæˆçš„èŒƒå›´åˆ—
        range_cols = [col for col in cleaned_df.columns if '_avg' in col]
        if range_cols:
            logger.info(f"æˆåŠŸç”Ÿæˆ {len(range_cols)} ä¸ªèŒƒå›´å¹³å‡å€¼åˆ—")
    
    # Step 5: åº”ç”¨å­—æ®µæ˜ å°„é…ç½®
    try:
        mapped_df = apply_field_mapping(cleaned_df)
        logger.info("å­—æ®µæ˜ å°„åº”ç”¨æˆåŠŸ")
        cleaned_df = mapped_df
    except Exception as e:
        logger.warning(f"å­—æ®µæ˜ å°„å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹å­—æ®µå")
    
    logger.info("ğŸ¯ æ•°æ®ç‚¼é‡‘å·¥åŠå®Œæˆ - æ•°æ®å·²ç‚¼æˆçº¯é‡‘ï¼")
    
    # åœ¨å‡½æ•°æœ€ç»ˆè¿”å›ä¹‹å‰è¿›è¡ŒéªŒè¯
    if cleaned_df is not None and not cleaned_df.is_empty():
        logger.info(f"--- [å¼•æ“å‡ºå£] æˆåŠŸå¤„ç†æ–‡ä»¶ï¼Œè¿”å› {cleaned_df.height} è¡Œæ•°æ® ---")
        return cleaned_df
    else:
        logger.error(f"--- [å¼•æ“å‡ºå£] å¤„ç†å¤±è´¥ï¼Œæœªèƒ½ç”Ÿæˆæœ‰æ•ˆæ•°æ®ï¼Œè¿”å› None ---")
        return None


def load_field_mapping_config() -> Dict:
    """
    åŠ è½½å­—æ®µæ˜ å°„é…ç½®æ–‡ä»¶
    
    Returns:
        Dict: é…ç½®å­—å…¸
    """
    config_path = Path(__file__).parent.parent / "config" / "field_map.yml"
    
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        logger.debug(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        return config
    except Exception as e:
        logger.warning(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return {}


def apply_field_mapping(df: pl.DataFrame) -> pl.DataFrame:
    """
    åº”ç”¨å­—æ®µæ˜ å°„é…ç½®
    
    å°†ä¸­æ–‡åˆ—åæ˜ å°„ä¸ºè‹±æ–‡æ ‡å‡†åˆ—å
    
    Args:
        df: åŸå§‹DataFrame
        
    Returns:
        pl.DataFrame: æ˜ å°„åçš„DataFrame
    """
    config = load_field_mapping_config()
    field_mapping = config.get('field_mapping', {})
    
    if not field_mapping:
        logger.warning("æœªæ‰¾åˆ°å­—æ®µæ˜ å°„é…ç½®ï¼Œä¿æŒåŸå§‹å­—æ®µå")
        return df
    
    # æ„å»ºé‡å‘½åæ˜ å°„
    rename_mapping = {}
    for col in df.columns:
        if col in field_mapping:
            new_name = field_mapping[col]
            rename_mapping[col] = new_name
            logger.debug(f"å­—æ®µæ˜ å°„: '{col}' -> '{new_name}'")
    
    if rename_mapping:
        df = df.rename(rename_mapping)
        logger.info(f"åº”ç”¨äº† {len(rename_mapping)} ä¸ªå­—æ®µæ˜ å°„")
    else:
        logger.info("æœªæ‰¾åˆ°åŒ¹é…çš„å­—æ®µæ˜ å°„")
    
    return df


def get_data_quality_report(df: pl.DataFrame) -> Dict:
    """
    ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š
    
    Args:
        df: æ•°æ®DataFrame
        
    Returns:
        Dict: è´¨é‡æŠ¥å‘Š
    """
    report = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'columns': df.columns,
        'null_counts': {},
        'data_types': {}
    }
    
    for col in df.columns:
        report['null_counts'][col] = df[col].null_count()
        report['data_types'][col] = str(df[col].dtype)
    
    return report


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    test_file = Path(__file__).parent.parent.parent / "data" / "test_sample.csv"
    
    if test_file.exists():
        result_df = process_douyin_export(test_file)
        print(f"å¤„ç†ç»“æœï¼š{len(result_df)} è¡Œ x {len(result_df.columns)} åˆ—")
        print(f"åˆ—åï¼š{result_df.columns}")
        
        # æ˜¾ç¤ºå‰å‡ è¡Œ
        print("\nå‰5è¡Œæ•°æ®é¢„è§ˆï¼š")
        print(result_df.head())
        
        # æ•°æ®è´¨é‡æŠ¥å‘Š
        quality_report = get_data_quality_report(result_df)
        print(f"\næ•°æ®è´¨é‡æŠ¥å‘Šï¼š{quality_report}")
    else:
        print(f"æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨ï¼š{test_file}")