"""
ğŸ¨ æ•°æ®ç‚¼é‡‘å·¥åŠ - Streamlit Webç•Œé¢
æç®€ä¼˜é›…çš„æŠ–éŸ³ç”µå•†æ•°æ®å¤„ç†ç•Œé¢

ä½œè€…: ç•Œé¢è®¾è®¡å¸ˆ
è®¾è®¡ç†å¿µ: ç”¨æˆ·ä¸éœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯Polarså’ŒDuckDBï¼Œä»–ä»¬åªéœ€è¦ä¸€ä¸ªä¸Šä¼ æŒ‰é’®å’Œä¸€ä¸ªä¸‹è½½æŒ‰é’®
"""

import streamlit as st
import polars as pl
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from pathlib import Path
import sys
import tempfile
import time
from datetime import datetime
import io
import traceback
import os
import zipfile

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from core.etl_douyin import process_douyin_export, get_data_quality_report

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ•°æ®ç‚¼é‡‘å·¥åŠ",
    page_icon="ğŸ—ï¸",
    layout="wide",
    initial_sidebar_state="expanded"
)

def init_page_style():
    """åˆå§‹åŒ–é¡µé¢æ ·å¼"""
    st.markdown("""
    <style>
    /* ä¸»é¢˜è‰²å½© */
    :root {
        --primary-color: #FF6B6B;
        --secondary-color: #4ECDC4;
        --accent-color: #45B7D1;
        --success-color: #96CEB4;
        --warning-color: #FFEAA7;
        --error-color: #DDA0DD;
    }
    
    /* éšè—é»˜è®¤èœå• */
    #MainMenu {visibility: hidden;}
    footer {visibility: hidden;}
    header {visibility: hidden;}
    
    /* è‡ªå®šä¹‰æ ‡é¢˜æ ·å¼ */
    .main-title {
        font-size: 3rem;
        font-weight: 700;
        background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        text-align: center;
        margin-bottom: 1rem;
    }
    
    .subtitle {
        font-size: 1.2rem;
        color: #666;
        text-align: center;
        margin-bottom: 2rem;
        font-style: italic;
    }
    
    /* å¡ç‰‡æ ·å¼ */
    .upload-card {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        padding: 2rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        color: white;
    }
    
    .result-card {
        background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
        padding: 2rem;
        border-radius: 15px;
        margin: 1rem 0;
        box-shadow: 0 8px 32px rgba(0,0,0,0.1);
        color: white;
    }
    
    .feature-card {
        background: white;
        padding: 1.5rem;
        border-radius: 10px;
        border-left: 4px solid var(--primary-color);
        margin: 0.5rem 0;
        box-shadow: 0 2px 10px rgba(0,0,0,0.1);
    }
    
    /* è¿›åº¦æ¡æ ·å¼ */
    .stProgress > div > div > div > div {
        background: linear-gradient(45deg, #FF6B6B, #4ECDC4);
    }
    
    /* æŒ‰é’®æ ·å¼ */
    .stDownloadButton > button {
        background: linear-gradient(45deg, #4ECDC4, #45B7D1);
        color: white;
        border: none;
        border-radius: 25px;
        padding: 0.5rem 2rem;
        font-weight: bold;
        transition: all 0.3s ease;
    }
    
    .stDownloadButton > button:hover {
        transform: translateY(-2px);
        box-shadow: 0 4px 15px rgba(0,0,0,0.2);
    }
    </style>
    """, unsafe_allow_html=True)

def render_header():
    """æ¸²æŸ“é¡µé¢å¤´éƒ¨"""
    st.markdown('<h1 class="main-title">ğŸ—ï¸ æ•°æ®ç‚¼é‡‘å·¥åŠ</h1>', unsafe_allow_html=True)
    st.markdown('<p class="subtitle">"å°†åŸå§‹æ•°æ®ç‚¼æˆçº¯å‡€çš„æ•°æ®é»„é‡‘"</p>', unsafe_allow_html=True)
    
    # åŠŸèƒ½ä»‹ç»å¡ç‰‡
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("""
        <div class="feature-card">
            <h4>ğŸš€ æ™ºèƒ½è§£æ</h4>
            <p>è‡ªåŠ¨è¯†åˆ«è‰å¦ˆå¦ˆå¤šè¡¨CSVç»“æ„</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col2:
        st.markdown("""
        <div class="feature-card">
            <h4>ğŸ”§ æ¨¡ç³Šå¤„ç†</h4>
            <p>å°†"7.5w~10w"è½¬æ¢ä¸ºç²¾ç¡®æ•°å€¼</p>
        </div>
        """, unsafe_allow_html=True)
    
    with col3:
        st.markdown("""
        <div class="feature-card">
            <h4>âš¡ æé€Ÿæ€§èƒ½</h4>
            <p>åŸºäºPolarsçš„æè‡´æ€§èƒ½ä¼˜åŒ–</p>
        </div>
        """, unsafe_allow_html=True)

def render_file_upload():
    """æ¸²æŸ“æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ - æ”¯æŒæ‰¹é‡ä¸Šä¼ """
    st.markdown("""
    <div class="upload-card">
        <h3>ğŸ“ ä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶</h3>
        <p>æ”¯æŒè‰å¦ˆå¦ˆå¯¼å‡ºçš„CSV/Excelæ–‡ä»¶ï¼Œå¯æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶</p>
    </div>
    """, unsafe_allow_html=True)
    
    # ç›´æ¥ä½¿ç”¨æ‰¹é‡ä¸Šä¼ æ¨¡å¼
    uploaded_files = st.file_uploader(
        "é€‰æ‹©æ–‡ä»¶ï¼ˆå¯å¤šé€‰ï¼‰",
        type=['csv', 'xlsx', 'xls'],
        accept_multiple_files=True,
        help="æ”¯æŒåŒæ—¶ä¸Šä¼ å¤šä¸ªæ–‡ä»¶è¿›è¡Œæ‰¹é‡å¤„ç†",
        label_visibility="collapsed"
    )
    return uploaded_files if uploaded_files else []

def render_processing_animation():
    """æ¸²æŸ“å¤„ç†åŠ¨ç”»"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    steps = [
        "ğŸ” æ™ºèƒ½è¯†åˆ«è¡¨æ ¼ç»“æ„...",
        "ğŸ“Š è§£æå¤šè¡¨æ•°æ®...",
        "ğŸ§¹ æ¸…æ´—æ¨¡ç³Šæ•°å€¼èŒƒå›´...", 
        "âš™ï¸ æ ‡å‡†åŒ–æ•°æ®æ ¼å¼...",
        "ğŸ¯ ç”Ÿæˆæ•°æ®è´¨é‡æŠ¥å‘Š...",
        "âœ¨ æ•°æ®ç‚¼é‡‘å®Œæˆï¼"
    ]
    
    for i, step in enumerate(steps):
        progress_bar.progress((i + 1) / len(steps))
        status_text.text(step)
        time.sleep(0.5)
    
    status_text.text("ğŸ‰ å¤„ç†å®Œæˆï¼")
    return progress_bar, status_text

def render_data_overview(df: pl.DataFrame, quality_report: dict):
    """æ¸²æŸ“æ•°æ®æ¦‚è§ˆ"""
    st.markdown("""
    <div class="result-card">
        <h3>ğŸ“Š æ•°æ®å¤„ç†ç»“æœ</h3>
        <p>æ‚¨çš„æ•°æ®å·²æˆåŠŸç‚¼åˆ¶å®Œæˆï¼Œå¯ä»¥å¼€å§‹åˆ†æäº†ï¼</p>
    </div>
    """, unsafe_allow_html=True)
    
    # åŸºç¡€ç»Ÿè®¡ä¿¡æ¯
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("æ€»è¡Œæ•°", f"{quality_report['total_rows']:,}", delta=None)
    
    with col2:
        st.metric("æ€»åˆ—æ•°", f"{quality_report['total_columns']}", delta=None)
    
    with col3:
        # è®¡ç®—æ•°å€¼åˆ—æ•°é‡
        numeric_cols = len([col for col in df.columns if df[col].dtype in [pl.Float64, pl.Int64]])
        st.metric("æ•°å€¼åˆ—", f"{numeric_cols}", delta=None)
    
    with col4:
        # è®¡ç®—ç”Ÿæˆçš„èŒƒå›´åˆ—æ•°é‡
        range_cols = len([col for col in df.columns if '_avg' in col])
        st.metric("ç”ŸæˆèŒƒå›´åˆ—", f"{range_cols}", delta=None)

def render_data_quality_chart(quality_report: dict):
    """æ¸²æŸ“æ•°æ®è´¨é‡å›¾è¡¨"""
    st.subheader("ğŸ“ˆ æ•°æ®è´¨é‡åˆ†æ")
    
    # ç©ºå€¼åˆ†æ
    null_data = quality_report['null_counts']
    if any(count > 0 for count in null_data.values()):
        null_df = pd.DataFrame([
            {"åˆ—å": col, "ç©ºå€¼æ•°é‡": count, "ç©ºå€¼æ¯”ä¾‹": count / quality_report['total_rows']}
            for col, count in null_data.items()
            if count > 0
        ])
        
        fig = px.bar(
            null_df, 
            x="åˆ—å", 
            y="ç©ºå€¼æ•°é‡",
            title="ç©ºå€¼åˆ†å¸ƒæƒ…å†µ",
            color="ç©ºå€¼æ¯”ä¾‹",
            color_continuous_scale="Reds"
        )
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.success("ğŸ‰ æ•°æ®è´¨é‡ä¼˜ç§€ï¼æ²¡æœ‰å‘ç°ç©ºå€¼ã€‚")
    
    # æ•°æ®ç±»å‹åˆ†æ
    type_data = quality_report['data_types']
    type_counts = {}
    for dtype in type_data.values():
        type_counts[dtype] = type_counts.get(dtype, 0) + 1
    
    fig_pie = px.pie(
        values=list(type_counts.values()),
        names=list(type_counts.keys()),
        title="æ•°æ®ç±»å‹åˆ†å¸ƒ"
    )
    st.plotly_chart(fig_pie, use_container_width=True)

def render_data_preview(df: pl.DataFrame):
    """æ¸²æŸ“æ•°æ®é¢„è§ˆ"""
    st.subheader("ğŸ‘€ æ•°æ®é¢„è§ˆ")
    
    # è½¬æ¢ä¸ºpandasä»¥ä¾¿åœ¨streamlitä¸­æ˜¾ç¤º
    df_pandas = df.to_pandas()
    
    # æ˜¾ç¤ºå‰å‡ è¡Œ
    st.write("**å‰5è¡Œæ•°æ®:**")
    st.dataframe(df_pandas.head(), use_container_width=True)
    
    # å¦‚æœæœ‰èŒƒå›´åˆ—ï¼Œå±•ç¤ºè½¬æ¢æ•ˆæœ
    range_columns = [col for col in df.columns if col.endswith('_avg')]
    if range_columns:
        st.write("**æ¨¡ç³Šæ•°å€¼èŒƒå›´å¤„ç†æ•ˆæœ:**")
        
        # æ‰¾åˆ°åŸå§‹åˆ—å’Œå¯¹åº”çš„èŒƒå›´åˆ—
        for avg_col in range_columns[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
            base_col = avg_col.replace('_avg', '')
            if base_col in df.columns:
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.write(f"**åŸå§‹**: {base_col}")
                    st.write(df_pandas[base_col].iloc[0] if len(df_pandas) > 0 else "æ— æ•°æ®")
                with col2:
                    st.write(f"**æœ€å°å€¼**: {avg_col.replace('_avg', '_min')}")
                    min_col = avg_col.replace('_avg', '_min')
                    if min_col in df.columns:
                        st.write(f"{df_pandas[min_col].iloc[0]:,.0f}" if len(df_pandas) > 0 and pd.notna(df_pandas[min_col].iloc[0]) else "N/A")
                with col3:
                    st.write(f"**æœ€å¤§å€¼**: {avg_col.replace('_avg', '_max')}")
                    max_col = avg_col.replace('_avg', '_max')
                    if max_col in df.columns:
                        st.write(f"{df_pandas[max_col].iloc[0]:,.0f}" if len(df_pandas) > 0 and pd.notna(df_pandas[max_col].iloc[0]) else "N/A")
                with col4:
                    st.write(f"**å¹³å‡å€¼**: {avg_col}")
                    st.write(f"{df_pandas[avg_col].iloc[0]:,.0f}" if len(df_pandas) > 0 and pd.notna(df_pandas[avg_col].iloc[0]) else "N/A")
                st.divider()

def create_download_file(df: pl.DataFrame, format="csv") -> bytes:
    """åˆ›å»ºä¸‹è½½æ–‡ä»¶ - ä¿®å¤ç‰ˆï¼šé»˜è®¤CSVæ ¼å¼ï¼Œå…¼å®¹Excel"""
    if format.lower() == "csv":
        # ğŸ”§ æ ¸å¿ƒä¿®å¤ï¼šä½¿ç”¨utf-8-sigç¼–ç ï¼Œç¡®ä¿Excelå…¼å®¹æ€§
        csv_string = df.write_csv()
        csv_bytes_with_bom = csv_string.encode('utf-8-sig')
        return csv_bytes_with_bom
    else:
        # Parquetæ ¼å¼ï¼ˆå¯é€‰ï¼‰
        buffer = io.BytesIO()
        df.write_parquet(buffer)
        buffer.seek(0)
        return buffer.getvalue()

def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–æ ·å¼
    init_page_style()
    
    # æ¸²æŸ“å¤´éƒ¨
    render_header()
    
    # ä¾§è¾¹æ ä¿¡æ¯
    with st.sidebar:
        st.markdown("### ğŸ¯ ä½¿ç”¨æŒ‡å—")
        st.markdown("""
        1. **ä¸Šä¼ æ–‡ä»¶**: æ”¯æŒCSV/Excelæ ¼å¼
        2. **è‡ªåŠ¨å¤„ç†**: ç³»ç»Ÿæ™ºèƒ½è¯†åˆ«å’Œæ¸…æ´—
        3. **æ‰¹é‡å¤„ç†**: æ”¯æŒå¤šæ–‡ä»¶åŒæ—¶å¤„ç†
        4. **ä¸‹è½½ç»“æœ**: CSVæ ¼å¼(å…¼å®¹Excel)
        
        ---
        """)
        
        st.markdown("### ğŸ”§ æŠ€æœ¯ç‰¹æ€§")
        st.markdown("""
        - ğŸš€ **æ™ºèƒ½å¼•æ“**: Polarsé«˜æ€§èƒ½å¤„ç†
        - ğŸ“ **æ‰¹é‡å¤„ç†**: å¤šæ–‡ä»¶åŒæ—¶å¤„ç†
        - ğŸ§  **æ™ºèƒ½è§£æ**: å¤šè¡¨è‡ªåŠ¨è¯†åˆ«
        - ğŸ”§ **æ¨¡ç³Šå¤„ç†**: æ™ºèƒ½æ•°å€¼èŒƒå›´è½¬æ¢
        - ğŸ“Š **åŸå§‹æ ¼å¼**: ä¿ç•™åŸå§‹æ•°æ®å±•ç¤º
        """)
    
    # ä¸»è¦å†…å®¹åŒºåŸŸ
    uploaded_files = render_file_upload()
    
    if uploaded_files:
        # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
        if len(uploaded_files) == 1:
            st.success(f"âœ… æ–‡ä»¶ä¸Šä¼ æˆåŠŸ: {uploaded_files[0].name}")
            st.info(f"ğŸ“ æ–‡ä»¶å¤§å°: {len(uploaded_files[0].getvalue()) / 1024:.1f} KB")
        else:
            st.success(f"âœ… æ‰¹é‡ä¸Šä¼ æˆåŠŸ: {len(uploaded_files)} ä¸ªæ–‡ä»¶")
            total_size = sum(len(f.getvalue()) for f in uploaded_files) / 1024
            st.info(f"ğŸ“ æ€»æ–‡ä»¶å¤§å°: {total_size:.1f} KB")
            
            # æ˜¾ç¤ºæ–‡ä»¶åˆ—è¡¨
            with st.expander("ğŸ“‹ æŸ¥çœ‹æ–‡ä»¶åˆ—è¡¨"):
                for i, f in enumerate(uploaded_files, 1):
                    st.write(f"{i}. {f.name} ({len(f.getvalue()) / 1024:.1f} KB)")
        
        # å¤„ç†æŒ‰é’® - ä¼˜åŒ–ç‰ˆæœ¬ï¼šæ‰¹é‡å¤„ç†ï¼Œä¸€æ¬¡æ€§ä¸‹è½½
        if st.button("ğŸš€ å¼€å§‹æ•°æ®ç‚¼é‡‘", type="primary", use_container_width=True):
            
            with st.spinner("ğŸ”§ æ•°æ®ç‚¼é‡‘å¼€å§‹ï¼æ­£åœ¨æ‰¹é‡å¤„ç†æ–‡ä»¶..."):
                processed_files = []  # å­˜å‚¨æ‰€æœ‰å¤„ç†æˆåŠŸçš„æ–‡ä»¶ä¿¡æ¯
                failed_files = []     # å­˜å‚¨å¤„ç†å¤±è´¥çš„æ–‡ä»¶
                
                # åˆ›å»ºè¿›åº¦æ¡
                progress_bar = st.progress(0)
                
                # æ‰¹é‡å¤„ç†æ‰€æœ‰æ–‡ä»¶
                for index, up_file in enumerate(uploaded_files):
                    progress_bar.progress((index + 1) / len(uploaded_files), 
                                        text=f"æ­£åœ¨å¤„ç† {index + 1}/{len(uploaded_files)}: {up_file.name}")
                    
                    try:
                        # 1. ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                        with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(up_file.name)[1]) as tmp_file:
                            tmp_file.write(up_file.getvalue())
                            tmp_file_path = tmp_file.name
                        
                        # 2. è°ƒç”¨æ ¸å¿ƒå¼•æ“å¤„ç†
                        cleaned_df = process_douyin_export(tmp_file_path)
                        
                        if cleaned_df is None:
                            failed_files.append(up_file.name)
                            continue
                        
                        # 3. è·å–åŸå§‹æ•°æ®ï¼ˆä»…ä¿å­˜ç¬¬ä¸€ä¸ªæ–‡ä»¶ç”¨äºå±•ç¤ºå¯¹æ¯”ï¼‰
                        original_df = None
                        if index == 0:  # åªè¯»å–ç¬¬ä¸€ä¸ªæ–‡ä»¶çš„åŸå§‹æ•°æ®
                            try:
                                # è¯»å–çœŸæ­£çš„åŸå§‹æ•°æ®
                                from app.core.etl_douyin import parse_messy_file
                                original_tables = parse_messy_file(tmp_file_path)
                                if original_tables:
                                    # è·å–ç¬¬ä¸€ä¸ªè¡¨æ ¼çš„åŸå§‹æ•°æ®
                                    first_table = list(original_tables.values())[0]
                                    original_df = first_table.head(5)
                                else:
                                    # é™çº§å¤„ç†ï¼šç›´æ¥è¯»å–æ–‡ä»¶
                                    if up_file.name.endswith('.csv'):
                                        original_df = pl.read_csv(tmp_file_path, encoding='utf-8-sig')
                                    else:
                                        original_df = pl.read_excel(tmp_file_path)
                                    original_df = original_df.head(5)
                            except:
                                # æœ€åé™çº§ï¼šä½¿ç”¨æ¸…æ´—åæ•°æ®ä½†ç§»é™¤_filteråˆ—
                                original_df = cleaned_df.drop([col for col in cleaned_df.columns if col.endswith('_filter')]).head(5)
                        
                        # ä¿å­˜å¤„ç†ç»“æœ
                        processed_files.append({
                            'name': up_file.name,
                            'cleaned_df': cleaned_df,  # åŒ…å«_filteråˆ—çš„å®Œæ•´æ•°æ®
                            'original_df': original_df if index == 0 else None,
                            'rows': len(cleaned_df),
                            'cols': len([col for col in cleaned_df.columns if not col.endswith('_filter')])  # åªè®¡ç®—åŸå§‹åˆ—æ•°
                        })
                        
                    except Exception as e:
                        failed_files.append(up_file.name)
                        
                    finally:
                        if 'tmp_file_path' in locals() and os.path.exists(tmp_file_path):
                            os.remove(tmp_file_path)
                
                progress_bar.empty()  # æ¸…é™¤è¿›åº¦æ¡
            
            # ä¿å­˜åˆ°session_stateä»¥ä¾¿ç­›é€‰ä½¿ç”¨
            if processed_files:
                st.session_state.processed_files = processed_files
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœ
            if processed_files:
                st.success(f"âœ… æˆåŠŸå¤„ç† {len(processed_files)}/{len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼")
                
                if failed_files:
                    with st.expander("æŸ¥çœ‹å¤±è´¥æ–‡ä»¶"):
                        for f in failed_files:
                            st.write(f"- {f}")
                
                # åªå±•ç¤ºç¬¬ä¸€ä¸ªæ–‡ä»¶çš„æ•°æ®å¯¹æ¯”
                if processed_files[0]['original_df'] is not None:
                    st.subheader("ğŸ“Š æ•°æ®å¯¹æ¯”é¢„è§ˆï¼ˆç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼‰")
                    st.caption(f"æ–‡ä»¶: {processed_files[0]['name']}")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.write("#### ğŸ“‹ åŸå§‹æ•°æ® (å‰5è¡Œ)")
                        # æ˜¾ç¤ºçœŸæ­£çš„åŸå§‹æ•°æ®
                        st.dataframe(processed_files[0]['original_df'].to_pandas(), use_container_width=True)
                    
                    with col2:
                        st.write("#### âœ¨ æ¸…æ´—åæ•°æ® (å‰5è¡Œ)")
                        # ç§»é™¤_filteråˆ—æ˜¾ç¤ºæ¸…æ´—åçš„æ•°æ®
                        display_df = processed_files[0]['cleaned_df'].drop(
                            [col for col in processed_files[0]['cleaned_df'].columns if col.endswith('_filter')]
                        )
                        st.dataframe(display_df.head(5).to_pandas(), use_container_width=True)
                
                # æ±‡æ€»ç»Ÿè®¡
                st.subheader("ğŸ“ˆ æ‰¹é‡å¤„ç†ç»Ÿè®¡")
                total_rows = sum(f['rows'] for f in processed_files)
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("æˆåŠŸæ–‡ä»¶æ•°", len(processed_files))
                with col2:
                    st.metric("å¤±è´¥æ–‡ä»¶æ•°", len(failed_files))
                with col3:
                    st.metric("æ€»æ•°æ®è¡Œæ•°", f"{total_rows:,}")
                with col4:
                    st.metric("å¹³å‡åˆ—æ•°", round(sum(f['cols'] for f in processed_files) / len(processed_files)))
                
                # æ‰¹é‡ä¸‹è½½åŠŸèƒ½
                st.subheader("ğŸ’¾ æ‰¹é‡ä¸‹è½½")
                
                # åˆ›å»ºZIPæ–‡ä»¶
                zip_buffer = io.BytesIO()
                with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                    for file_info in processed_files:
                        # ç§»é™¤_filteråˆ—åå†ä¿å­˜
                        export_df = file_info['cleaned_df'].drop(
                            [col for col in file_info['cleaned_df'].columns if col.endswith('_filter')]
                        )
                        csv_content = export_df.write_csv()
                        csv_bytes = csv_content.encode('utf-8-sig')
                        zip_file.writestr(f"cleaned_{file_info['name']}", csv_bytes)
                
                zip_buffer.seek(0)
                
                # æä¾›ZIPä¸‹è½½æŒ‰é’®
                st.download_button(
                    label=f"â¬‡ï¸ ä¸€é”®ä¸‹è½½æ‰€æœ‰æ¸…æ´—åçš„æ–‡ä»¶ ({len(processed_files)}ä¸ªæ–‡ä»¶)",
                    data=zip_buffer.getvalue(),
                    file_name=f"cleaned_batch_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip",
                    mime="application/zip",
                    use_container_width=True,
                    help="ZIPå‹ç¼©åŒ…ï¼ŒåŒ…å«æ‰€æœ‰å¤„ç†æˆåŠŸçš„æ–‡ä»¶"
                )
                
                # æä¾›å•ä¸ªæ–‡ä»¶ä¸‹è½½é€‰é¡¹
                with st.expander("å•ç‹¬ä¸‹è½½æ–‡ä»¶"):
                    for idx, file_info in enumerate(processed_files):
                        # ç§»é™¤_filteråˆ—åå†æä¾›ä¸‹è½½
                        export_df = file_info['cleaned_df'].drop(
                            [col for col in file_info['cleaned_df'].columns if col.endswith('_filter')]
                        )
                        csv_string = export_df.write_csv()
                        csv_bytes = csv_string.encode('utf-8-sig')
                        
                        st.download_button(
                            label=f"ğŸ“¥ {file_info['name']} ({file_info['rows']} è¡Œ)",
                            data=csv_bytes,
                            file_name=f"cleaned_{file_info['name']}",
                            mime="text/csv",
                            key=f"single-download-{idx}"
                        )
            else:
                st.error("âŒ æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®ã€‚")
    


def process_single_file(uploaded_file) -> pl.DataFrame:
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
        file_extension = os.path.splitext(uploaded_file.name)[1]
        with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        # æ˜¾ç¤ºå¤„ç†åŠ¨ç”»
        with st.spinner("æ­£åœ¨ç‚¼åˆ¶æ‚¨çš„æ•°æ®..."):
            progress_bar, status_text = render_processing_animation()
        
        try:
            # è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
            result_df = process_douyin_export(tmp_file_path)
            return result_df
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(tmp_file_path)
    
    except Exception as e:
        st.error(f"âŒ å¤„ç†æ–‡ä»¶ {uploaded_file.name} å¤±è´¥: {str(e)}")
        st.exception(e)
        return None


def process_multiple_files(uploaded_files) -> pl.DataFrame:
    """æ‰¹é‡å¤„ç†å¤šä¸ªæ–‡ä»¶"""
    processed_dfs = []
    failed_files = []
    
    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_container = st.container()
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            with status_container:
                st.write(f"ğŸ”„ æ­£åœ¨å¤„ç†æ–‡ä»¶ {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
            
            # æ›´æ–°è¿›åº¦æ¡
            progress_bar.progress((i + 1) / len(uploaded_files))
            
            # ä¿å­˜æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
            file_extension = os.path.splitext(uploaded_file.name)[1]
            with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            try:
                # å¤„ç†æ–‡ä»¶
                result_df = process_douyin_export(tmp_file_path)
                
                # æ·»åŠ æ–‡ä»¶æ¥æºåˆ—
                result_df = result_df.with_columns([
                    pl.lit(uploaded_file.name).alias("æ–‡ä»¶æ¥æº")
                ])
                
                processed_dfs.append(result_df)
                
                with status_container:
                    st.write(f"âœ… {uploaded_file.name} å¤„ç†å®Œæˆ: {len(result_df)} è¡Œæ•°æ®")
                
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.remove(tmp_file_path)
                
        except Exception as e:
            failed_files.append(uploaded_file.name)
            with status_container:
                st.write(f"âŒ {uploaded_file.name} å¤„ç†å¤±è´¥: {str(e)}")
    
    # åˆå¹¶æ‰€æœ‰å¤„ç†æˆåŠŸçš„DataFrame
    if processed_dfs:
        try:
            # ä½¿ç”¨Polarsçš„concatå‡½æ•°åˆå¹¶
            merged_df = pl.concat(processed_dfs, how="diagonal")
            
            success_count = len(processed_dfs)
            total_rows = len(merged_df)
            
            st.success(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {success_count}/{len(uploaded_files)} ä¸ªæ–‡ä»¶ï¼Œå…± {total_rows} è¡Œæ•°æ®")
            
            if failed_files:
                st.warning(f"âš ï¸ ä»¥ä¸‹æ–‡ä»¶å¤„ç†å¤±è´¥: {', '.join(failed_files)}")
            
            return merged_df
            
        except Exception as e:
            st.error(f"âŒ åˆå¹¶æ•°æ®å¤±è´¥: {str(e)}")
            return None
    else:
        st.error("âŒ æ²¡æœ‰ä»»ä½•æ–‡ä»¶å¤„ç†æˆåŠŸ")
        return None
    

if __name__ == "__main__":
    main()