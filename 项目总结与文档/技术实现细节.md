# 技术实现细节文档

## 核心技术架构

### 1. 数据处理引擎 (ETL Core)

#### 1.1 文件解析模块
**文件**: `app/core/etl_douyin.py`

```python
def parse_messy_file(file_path: Union[str, Path]) -> Dict[str, pl.DataFrame]:
    """
    智能解析混乱的文件 - 核心算法
    
    技术要点:
    1. 自动检测文件类型 (CSV/Excel)
    2. 智能编码识别 (UTF-8-SIG, GBK, GB2312)
    3. 多表结构自动分离
    4. 表头关键词匹配算法
    """
```

**关键算法**:
- **编码检测**: 使用chardet + 候选编码列表
- **分隔符识别**: 统计分析制表符、逗号等
- **表头识别**: 20+关键词匹配 + 非空单元格占比
- **表格分离**: 基于表头位置的区间切分

#### 1.2 模糊数值解析引擎
```python
def parse_fuzzy_numeric_range(series: pl.Series) -> Dict[str, pl.Series]:
    """
    模糊数值范围解析 - 核心算法
    
    支持格式:
    - "7.5w~10w" → min:75000, max:100000, avg:87500
    - "20.00%" → 0.2
    - "346.95万" → 3469500
    """
```

**解析规则**:
```python
UNIT_MAP = {
    "w": 10_000,     # 万
    "万": 10_000,
    "k": 1_000,      # 千
    "千": 1_000
}

# 正则表达式匹配
RANGE_PATTERN = r"([0-9.]+)\s*([w万k千%]?)\s*[~～-]\s*([0-9.]+)\s*([w万k千%]?)"
SINGLE_PATTERN = r"([0-9.]+)\s*([w万k千%]?)"
```

#### 1.3 数据清洗核心逻辑

**Version 1.0 (已废弃)**:
```python
# 旧版本 - 创建多列
def clean_common_fields_old(df: pl.DataFrame) -> pl.DataFrame:
    for field in range_fields:
        range_data = parse_fuzzy_numeric_range(df[field])
        df = df.with_columns([
            range_data['min'].alias(f'{field}_min'),
            range_data['max'].alias(f'{field}_max'),
            range_data['avg'].alias(f'{field}_avg')
        ])
        df = df.drop(field)  # 删除原始列
    return df
```

**Version 2.0 (当前版本)**:
```python
# 新版本 - 保留原始数据 + _filter列
def clean_common_fields(df: pl.DataFrame) -> pl.DataFrame:
    cleaned_df = df.clone()  # 保留所有原始列
    
    for field in range_fields:
        if field in df.columns:
            range_data = parse_fuzzy_numeric_range(df[field])
            # 只创建_filter列存储下限值
            cleaned_df = cleaned_df.with_columns([
                range_data['min'].alias(f'{field}_filter')
            ])
    
    return cleaned_df  # 原始列 + _filter列
```

### 2. Web界面架构 (Streamlit)

#### 2.1 界面设计系统
**文件**: `app/ui/streamlit_app.py`

**设计理念**: 极简美学 + 渐变色彩
```css
/* 主题色彩系统 */
:root {
    --primary-color: #FF6B6B;     /* 主色调 */
    --secondary-color: #4ECDC4;   /* 辅助色 */
    --accent-color: #45B7D1;      /* 强调色 */
    --success-color: #96CEB4;     /* 成功色 */
    --warning-color: #FFEAA7;     /* 警告色 */
    --error-color: #DDA0DD;       /* 错误色 */
}

/* 渐变卡片 */
.upload-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border-radius: 15px;
    box-shadow: 0 8px 32px rgba(0,0,0,0.1);
}
```

#### 2.2 状态管理系统
```python
# Session State 管理
if 'processed_files' not in st.session_state:
    st.session_state.processed_files = []

if 'filtered_results' not in st.session_state:
    st.session_state.filtered_results = None

# 文件处理状态跟踪
st.session_state.current_file = uploaded_file.name
st.session_state.original_df = df_original
st.session_state.processed_df = process_dataframe(df_original)
```

#### 2.3 数据筛选UI实现
```python
# 动态生成筛选控件
def render_filter_sidebar():
    filter_columns = [col.replace('_filter', '') 
                     for col in df.columns 
                     if col.endswith('_filter')]
    
    filters = {}
    for col in filter_columns:
        filter_col = f"{col}_filter"
        min_val = float(df[filter_col].min())
        max_val = float(df[filter_col].max())
        
        # 滑块控件
        selected_range = st.slider(
            f"{col}",
            min_val,
            max_val,
            (min_val, max_val),
            key=f"filter_{col}"
        )
        filters[filter_col] = selected_range
    
    return filters
```

### 3. 数据流转架构

#### 3.1 完整数据流
```
用户上传文件
    ↓
智能编码检测 (detect_encoding)
    ↓
文件格式识别 (detect_file_type)
    ↓
多表结构解析 (parse_messy_file)
    ↓
表头识别算法 (_detect_tables_in_sheet)
    ↓
数据清洗处理 (clean_common_fields)
    ↓
添加_filter列 (parse_fuzzy_numeric_range)
    ↓
UI展示原始数据 (移除_filter列)
    ↓
用户筛选操作 (基于_filter列)
    ↓
筛选结果展示 (移除_filter列)
    ↓
数据下载导出 (移除_filter列)
```

#### 3.2 错误处理机制
```python
# 多级错误处理
try:
    # 1. 文件读取错误
    df = pl.read_excel(file_path)
except Exception as e:
    logger.error(f"文件读取失败: {e}")
    try:
        # 2. 降级到CSV处理
        df = pl.read_csv(file_path, encoding='utf-8-sig')
    except Exception as e2:
        logger.error(f"CSV读取也失败: {e2}")
        return None

# 3. 数据验证
if df.is_empty():
    raise ValueError("数据为空")

# 4. 关键列检查
required_columns = ["商品", "销量"]
missing_columns = [col for col in required_columns if col not in df.columns]
if missing_columns:
    logger.warning(f"缺少关键列: {missing_columns}")
```

### 4. 性能优化技术

#### 4.1 Polars优化策略
```python
# 懒惰执行 - 查询优化
lazy_df = df.lazy()
result = lazy_df.filter(
    pl.col("销量_filter") > 10000
).select([
    pl.col("商品"),
    pl.col("销量"),
    pl.col("销量_filter")
]).collect()

# 零拷贝操作
cleaned_df = df.clone()  # 共享内存，不复制数据

# 批量列操作
df = df.with_columns([
    pl.col("佣金比例").str.replace("%", "").cast(pl.Float64).alias("佣金比例_filter"),
    pl.col("销量").str.replace("w", "").cast(pl.Float64).mul(10000).alias("销量_filter")
])
```

#### 4.2 内存管理
```python
# 临时文件清理
try:
    with tempfile.NamedTemporaryFile(delete=False, suffix='.xlsx') as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name
    
    # 处理文件
    result = process_douyin_export(tmp_file_path)
    
finally:
    # 确保临时文件被删除
    if os.path.exists(tmp_file_path):
        os.remove(tmp_file_path)
```

#### 4.3 批量处理优化
```python
# 并行处理多文件
processed_files = []
with st.spinner("批量处理中..."):
    for idx, file in enumerate(uploaded_files):
        # 进度显示
        progress_bar.progress((idx + 1) / len(uploaded_files))
        
        # 异步处理
        result = process_file_async(file)
        processed_files.append(result)

# 内存友好的ZIP创建
zip_buffer = io.BytesIO()
with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    for file_info in processed_files:
        # 移除_filter列后再压缩
        export_df = file_info['df'].drop([col for col in file_info['df'].columns if col.endswith('_filter')])
        csv_content = export_df.write_csv()
        zip_file.writestr(f"cleaned_{file_info['name']}", csv_content.encode('utf-8-sig'))
```

### 5. 测试架构

#### 5.1 单元测试设计
```python
# test_etl_douyin.py
class TestETLDouyin:
    def test_parse_fuzzy_numeric_range(self):
        """测试模糊数值解析"""
        test_data = pl.Series(["7.5w~10w", "2.5w", "20%", "1000"])
        result = parse_fuzzy_numeric_range(test_data)
        
        assert result['min'][0] == 75000.0
        assert result['max'][0] == 100000.0
        assert result['avg'][0] == 87500.0
    
    def test_clean_common_fields_preserves_original(self):
        """测试原始数据保留"""
        original_df = pl.DataFrame({
            "商品": ["测试商品"],
            "销量": ["7.5w~10w"]
        })
        
        cleaned_df = clean_common_fields(original_df)
        
        # 验证原始列保留
        assert "商品" in cleaned_df.columns
        assert "销量" in cleaned_df.columns
        assert cleaned_df["销量"][0] == "7.5w~10w"
        
        # 验证_filter列创建
        assert "销量_filter" in cleaned_df.columns
        assert cleaned_df["销量_filter"][0] == 75000.0
```

#### 5.2 集成测试
```python
def test_complete_workflow():
    """端到端工作流测试"""
    # 1. 文件上传模拟
    test_file_path = "data/test_sample.csv"
    
    # 2. 完整处理流程
    result_df = process_douyin_export(test_file_path)
    
    # 3. 验证结果
    assert result_df is not None
    assert len(result_df) > 0
    
    # 4. 验证_filter列
    filter_cols = [col for col in result_df.columns if col.endswith('_filter')]
    assert len(filter_cols) > 0
    
    # 5. 模拟筛选操作
    filtered_df = result_df.filter(pl.col("销量_filter") > 50000)
    
    # 6. 验证输出格式
    export_df = filtered_df.drop(filter_cols)
    assert "销量_filter" not in export_df.columns
```

### 6. 部署配置

#### 6.1 Docker配置
```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    software-properties-common \
    git \
    && rm -rf /var/lib/apt/lists/*

# 复制项目文件
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .

# 暴露端口
EXPOSE 8502

# 健康检查
HEALTHCHECK CMD curl --fail http://localhost:8502/_stcore/health

# 启动命令
ENTRYPOINT ["streamlit", "run", "app/ui/streamlit_app.py", "--server.port=8502", "--server.address=0.0.0.0"]
```

#### 6.2 启动脚本
```bash
#!/bin/bash
# run_web.sh

echo "--- 准备启动数据炼金工坊 Web 界面 ---"

# 激活虚拟环境
source venv/bin/activate

# 环境变量设置
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
export STREAMLIT_SERVER_PORT=8502
export STREAMLIT_BROWSER_GATHER_USAGE_STATS=false

# 清理旧进程
pkill -f "streamlit.*8502" 2>/dev/null || true

# 后台启动
nohup streamlit run app/ui/streamlit_app.py \
    --server.port=8502 \
    --server.address=0.0.0.0 \
    --server.headless=true \
    --browser.gatherUsageStats=false \
    > streamlit.log 2>&1 &

PID=$!
echo "Streamlit 进程ID: $PID"
echo "🌐 请在浏览器中访问: http://localhost:8502"
```

### 7. 监控与日志

#### 7.1 日志系统
```python
import logging

# 配置日志
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# 关键操作日志
logger.info(f"开始处理文件: {file_path}")
logger.info(f"检测到文件类型: {file_type}")
logger.info(f"成功解析 {len(tables)} 个表格")
logger.warning(f"跳过字段 {field} 的范围解析")
logger.error(f"处理失败: {error}")
```

#### 7.2 性能监控
```python
import time
from functools import wraps

def monitor_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        logger.info(f"{func.__name__} 执行时间: {end_time - start_time:.4f}秒")
        return result
    return wrapper

@monitor_performance
def process_douyin_export(file_path):
    # 处理逻辑
    pass
```

### 8. 安全性考虑

#### 8.1 文件上传安全
```python
ALLOWED_EXTENSIONS = {'.csv', '.xlsx', '.xls'}
MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

def validate_file(uploaded_file):
    # 文件扩展名检查
    file_ext = os.path.splitext(uploaded_file.name)[1].lower()
    if file_ext not in ALLOWED_EXTENSIONS:
        raise ValueError(f"不支持的文件类型: {file_ext}")
    
    # 文件大小检查
    if len(uploaded_file.getvalue()) > MAX_FILE_SIZE:
        raise ValueError("文件过大，超过50MB限制")
    
    # 文件内容检查
    if len(uploaded_file.getvalue()) == 0:
        raise ValueError("文件为空")
```

#### 8.2 数据隐私保护
```python
# 临时文件安全删除
import os
import tempfile

def secure_temp_file_processing(uploaded_file):
    try:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.tmp') as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_file_path = tmp_file.name
        
        # 设置文件权限
        os.chmod(tmp_file_path, 0o600)  # 只有所有者可读写
        
        # 处理文件
        result = process_file(tmp_file_path)
        
        return result
        
    finally:
        # 安全删除临时文件
        if os.path.exists(tmp_file_path):
            # 覆盖文件内容
            with open(tmp_file_path, 'wb') as f:
                f.write(b'\x00' * os.path.getsize(tmp_file_path))
            os.remove(tmp_file_path)
```

---

**文档版本**: v2.0.0  
**最后更新**: 2025年6月21日  
**维护者**: Claude Code + 技术团队