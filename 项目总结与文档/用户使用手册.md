# 数据炼金工坊 - 用户使用手册

## 快速开始

### 系统要求
- **操作系统**: Windows 10+, macOS 10.15+, Ubuntu 18.04+
- **Python版本**: 3.9+
- **浏览器**: Chrome, Firefox, Safari, Edge (最新版本)
- **内存**: 建议4GB以上
- **存储**: 至少1GB可用空间

### 安装与启动

#### 方式一：本地运行
```bash
# 1. 进入项目目录
cd dy-ec-cleaner

# 2. 激活虚拟环境
source venv/bin/activate  # Linux/Mac
# 或
venv\Scripts\activate     # Windows

# 3. 启动Web界面
./run_web.sh              # Linux/Mac
# 或
python -m streamlit run app/ui/streamlit_app.py --server.port=8502

# 4. 在浏览器中访问
http://localhost:8502
```

#### 方式二：Docker运行
```bash
# 1. 构建镜像
docker build -t dy-ec-cleaner .

# 2. 运行容器
docker run -p 8502:8502 dy-ec-cleaner

# 3. 访问界面
http://localhost:8502
```

## 界面介绍

### 主界面布局
```
┌─────────────────────────────────────────────────────────────┐
│                    🏗️ 数据炼金工坊                          │
│                "将原始数据炼成纯净的数据黄金"                │
├─────────────────────────────────────────────────────────────┤
│  📁 上传您的数据文件                                        │
│  支持蝉妈妈导出的CSV/Excel文件，可批量处理多个文件          │
│  [选择文件(可多选)] [🚀 开始数据炼金]                       │
├─────────────────────────────────────────────────────────────┤
│  📊 数据对比预览                                            │
│  ┌─────────────────┬─────────────────┐                      │
│  │ 📋 原始数据     │ ✨ 清洗后数据   │                      │
│  │ (前5行)         │ (前5行)         │                      │
│  └─────────────────┴─────────────────┘                      │
├─────────────────────────────────────────────────────────────┤
│  📥 批量下载                                                │
│  [⬇️ 一键下载所有文件] [单独下载文件▼]                     │
└─────────────────────────────────────────────────────────────┘
```

### 侧边栏功能
```
┌─────────────────────┐
│ 🎯 使用指南         │
│ ─────────────────── │
│ 🔍 数据筛选         │
│ ├ 选择文件: [下拉]  │
│ ├ 佣金比例: [滑块]  │
│ ├ 近30天销量: [滑块]│
│ ├ 转化率: [滑块]    │
│ └ [应用筛选]        │
│ ─────────────────── │
│ 🔧 技术特性         │
│ └ 📞 技术支持       │
└─────────────────────┘
```

## 功能详解

### 1. 文件上传功能

#### 支持的文件格式
- **CSV文件** (.csv)
  - 编码：UTF-8、UTF-8-SIG、GBK、GB2312
  - 分隔符：逗号、制表符、分号
  - 最大大小：50MB

- **Excel文件** (.xlsx, .xls)
  - 支持多工作表
  - 自动检测表格结构
  - 最大大小：50MB

#### 上传步骤
1. 点击"选择文件(可多选)"按钮
2. 在文件选择器中选择一个或多个文件
3. 界面会显示文件信息和总大小
4. 点击"🚀 开始数据炼金"开始处理

### 2. 数据处理功能

#### 支持的数据表格类型
系统能自动识别以下6种蝉妈妈数据表格：

1. **SKU商品库**
   - 包含：商品信息、销量、销售额、佣金比例
   - 关键列：商品、商品链接、商品分类、近30天销量

2. **抖音销量榜**
   - 包含：排名、商品、蝉妈妈链接、销量
   - 关键列：排名、商品、周销量、销售额

3. **抖音热推榜**
   - 包含：排名、商品、带货达人、转化率
   - 关键列：排名、商品、周带货达人、30天转化率

4. **潜力爆品榜**
   - 包含：排名、商品、佣金比例、昨日销量
   - 关键列：排名、商品、佣金比例、昨日销量

5. **持续好货榜**
   - 包含：排名、商品、近90天销量
   - 关键列：排名、商品、近90天销量

6. **历史同期榜**
   - 包含：排名、商品、同期销量
   - 关键列：排名、商品、同期销量

#### 数据清洗规则

**模糊数值范围处理**：
```
输入格式               输出结果
"7.5w~10w"     →      保留原始显示 + 创建隐藏筛选列(75000)
"2.5万~5万"    →      保留原始显示 + 创建隐藏筛选列(25000)
"10%~15%"      →      保留原始显示 + 创建隐藏筛选列(0.10)
"5000"         →      保留原始显示 + 创建隐藏筛选列(5000)
```

**佣金比例处理**：
```
"20.00%"       →      保留原始显示 + 创建隐藏筛选列(0.20)
"15%"          →      保留原始显示 + 创建隐藏筛选列(0.15)
```

**单位换算规则**：
- `w` / `万` = × 10,000
- `k` / `千` = × 1,000
- `%` = ÷ 100 (转为小数)

### 3. 数据筛选功能

#### 筛选原理
- 系统为每个范围列创建隐藏的`_filter`列
- `_filter`列存储范围的下限值用于数值比较
- 用户界面基于`_filter`列生成滑块控件
- 筛选结果显示时移除所有`_filter`列

#### 使用步骤
1. **选择文件**（如果处理了多个文件）
2. **设置筛选条件**
   - 使用滑块调整各个指标的范围
   - 可以同时设置多个筛选条件
3. **应用筛选**
   - 点击"应用筛选"按钮
   - 系统会显示筛选统计信息
4. **查看结果**
   - 筛选后的数据会显示在主界面
   - 提供独立的下载按钮

#### 筛选示例
假设有数据：
```
商品A: 近30天销量 "7.5w~10w"  → 筛选值: 75000
商品B: 近30天销量 "2.5w~5w"   → 筛选值: 25000
商品C: 近30天销量 "15w~20w"   → 筛选值: 150000
```

设置筛选条件：近30天销量 >= 50000
结果：只显示商品A和商品C

### 4. 数据下载功能

#### 批量下载
- **一键下载所有文件**
  - 格式：ZIP压缩包
  - 文件名：`cleaned_batch_时间戳.zip`
  - 内容：所有处理成功的文件（原始格式）

#### 单独下载
- **每个文件独立下载**
  - 格式：CSV文件
  - 编码：UTF-8-SIG（兼容Excel）
  - 文件名：`cleaned_原文件名.csv`

#### 筛选结果下载
- **筛选后数据下载**
  - 格式：CSV文件
  - 文件名：`filtered_原文件名.csv`
  - 内容：只包含符合筛选条件的数据（原始格式）

## 常见使用场景

### 场景1：单文件数据清洗
**适用情况**：处理单个蝉妈妈导出文件

**操作步骤**：
1. 上传CSV或Excel文件
2. 等待处理完成
3. 查看数据对比预览
4. 下载清洗后的数据

**预期结果**：
- 原始数据格式完全保留
- 可以在Excel中正常打开
- 数据结构清晰易读

### 场景2：批量文件处理
**适用情况**：同时处理多个文件

**操作步骤**：
1. 选择多个文件上传
2. 点击"开始数据炼金"
3. 查看处理进度和统计
4. 使用"一键下载所有文件"

**预期结果**：
- 所有文件统一处理
- ZIP包包含所有清洗后的文件
- 提供处理成功/失败统计

### 场景3：数据筛选分析
**适用情况**：需要根据条件筛选数据

**操作步骤**：
1. 上传并处理文件
2. 在侧边栏设置筛选条件
3. 点击"应用筛选"
4. 查看筛选结果和统计
5. 下载筛选后的数据

**预期结果**：
- 精确的条件筛选
- 实时的数据统计
- 筛选结果保持原始格式

### 场景4：数据质量检查
**适用情况**：验证数据处理效果

**操作步骤**：
1. 上传文件并处理
2. 对比原始数据和处理后数据
3. 检查数据转换是否正确
4. 验证筛选功能

**预期结果**：
- 清晰的前后对比
- 原始数据完全保留
- 筛选功能准确可靠

## 故障排除

### 常见问题及解决方案

#### 1. 文件上传失败
**问题现象**：
- 文件上传后无响应
- 显示"文件格式不支持"
- 上传进度停滞

**解决方案**：
1. **检查文件格式**
   ```
   支持格式：.csv, .xlsx, .xls
   最大大小：50MB
   ```

2. **检查文件编码**
   ```
   推荐编码：UTF-8, UTF-8-SIG
   避免使用：ANSI, ASCII
   ```

3. **检查文件内容**
   - 确保文件不为空
   - 确保包含有效的表格数据
   - 检查是否有特殊字符

#### 2. 数据处理失败
**问题现象**：
- 处理进度条停滞
- 显示"无法解析有效数据"
- 处理结果为空

**解决方案**：
1. **检查数据格式**
   ```bash
   # 查看错误日志
   tail -f streamlit.log
   ```

2. **验证表头信息**
   - 确保包含关键列（如"商品"、"销量"）
   - 检查列名是否正确
   - 验证数据类型

3. **简化数据测试**
   - 使用提供的测试样本
   - 逐步增加数据复杂度

#### 3. 筛选功能异常
**问题现象**：
- 筛选滑块不出现
- 筛选结果不正确
- 筛选后数据为空

**解决方案**：
1. **检查数据类型**
   - 确保包含数值型的范围数据
   - 验证模糊数值格式（如"1w~2w"）

2. **重置筛选条件**
   - 刷新页面重新开始
   - 检查筛选范围是否合理

3. **检查浏览器兼容性**
   - 使用Chrome或Firefox最新版本
   - 清除浏览器缓存

#### 4. 下载功能问题
**问题现象**：
- 下载文件为空
- CSV文件在Excel中乱码
- ZIP文件损坏

**解决方案**：
1. **编码问题**
   ```
   下载的CSV使用UTF-8-SIG编码
   可以直接在Excel中打开
   如果仍有乱码，尝试：
   Excel → 数据 → 获取数据 → 从文本/CSV
   ```

2. **文件完整性**
   - 等待处理完全完成再下载
   - 检查网络连接稳定性

3. **浏览器设置**
   - 检查下载文件夹权限
   - 关闭下载拦截器

### 日志查看
```bash
# 查看实时日志
tail -f streamlit.log

# 查看错误信息
grep ERROR streamlit.log

# 查看处理统计
grep "处理完成" streamlit.log
```

### 性能优化建议

1. **文件大小控制**
   - 单个文件建议不超过10MB
   - 批量处理建议不超过5个文件

2. **浏览器优化**
   - 关闭不必要的浏览器标签
   - 使用Chrome或Firefox最新版本

3. **系统资源**
   - 确保足够的内存空间
   - 关闭其他大型应用程序

## 高级功能

### 命令行工具
```bash
# 直接处理文件
python -m app.cli.main process input.csv -o output.csv

# 批量处理目录
python datacleaner.py --input-dir data/ --output-dir results/

# 查看帮助
python -m app.cli.main --help
```

### API接口
```python
# Python API调用
from app.core.etl_douyin import process_douyin_export

result_df = process_douyin_export("your_file.csv")
print(f"处理结果：{len(result_df)} 行数据")
```

### 配置自定义
```yaml
# app/config/field_map.yml
field_mapping:
  "近30天销量": "sales_30d"
  "佣金比例": "commission_rate"
  
# 添加新的字段映射
custom_fields:
  "自定义字段": "custom_field"
```

## 技术支持

### 联系方式
- **GitHub Issues**: https://github.com/qinshu1109/sjgl/issues
- **技术文档**: 项目根目录下的README.md
- **API文档**: /docs目录

### 开发团队
- **管道架构师**: 核心引擎开发
- **界面设计师**: UI/UX设计
- **整合工程师**: 系统部署与维护

### 版本更新
- **当前版本**: v2.0.0
- **更新频率**: 根据用户反馈持续优化
- **获取更新**: `git pull origin main`

---

**手册版本**: v2.0.0  
**最后更新**: 2025年6月21日  
**适用版本**: 数据炼金工坊 v2.0.0+